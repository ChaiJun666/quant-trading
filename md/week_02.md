
## ğŸ“– ï¼ˆDay 8ï¼‰AIé‡åŒ–å­¦ä¹ æ—¥å¿—ï¼šRSI æŠ€æœ¯æŒ‡æ ‡ä¸è‚¡ä»·é¢„æµ‹

---

### ğŸ“Œ ä»Šæ—¥è®¡åˆ’

* **ä»Šæ—¥ç›®æ ‡**ï¼šå­¦ä¹  RSIï¼ˆç›¸å¯¹å¼ºå¼±æŒ‡æ ‡ï¼‰çš„è®¡ç®—æ–¹æ³•ï¼Œå¹¶ä½œä¸ºæ–°ç‰¹å¾åŠ å…¥é¢„æµ‹æ¨¡å‹ã€‚
* **å­¦ä¹ å†…å®¹**ï¼š

  1. RSI çš„åŸºæœ¬åŸç†ä¸å…¬å¼
  2. Python å®ç° RSI æŒ‡æ ‡è®¡ç®—
  3. å°† RSI ç‰¹å¾åŠ å…¥å›å½’æ¨¡å‹ï¼Œè§‚å¯Ÿæ•ˆæœå˜åŒ–

---

### ğŸ› ï¸ å­¦ä¹ è¿‡ç¨‹

#### 1. ç†è®ºå­¦ä¹ 

* **RSIï¼ˆRelative Strength Indexï¼‰**

  * æ ¸å¿ƒæ€æƒ³ï¼šé€šè¿‡æ¶¨è·Œå¹…æ¥è¡¡é‡è‚¡ç¥¨çš„è¶…ä¹°/è¶…å–çŠ¶æ€ã€‚
  * å…¬å¼ï¼š

    $$
    RSI = 100 - \frac{100}{1 + RS}
    $$

    å…¶ä¸­ï¼Œ
    \$RS = \frac{\text{å¹³å‡æ¶¨å¹…}}{\text{å¹³å‡è·Œå¹…}}\$
  * å¸¸è§å‘¨æœŸï¼š14 æ—¥ RSI
  * è§£é‡Šï¼š

    * RSI > 70 â†’ è¶…ä¹°ï¼ˆå¯èƒ½ä¸‹è·Œï¼‰
    * RSI < 30 â†’ è¶…å–ï¼ˆå¯èƒ½åå¼¹ï¼‰

* RSI æ˜¯ä¸€ç§å¸¸ç”¨çš„ **åŠ¨é‡æŒ‡æ ‡**ï¼Œèƒ½åæ˜ çŸ­æœŸä»·æ ¼èµ°åŠ¿ã€‚

---

#### 2. Python å®è·µ

æˆ‘åœ¨æ˜¨å¤©çš„ã€Œæ”¶ç›˜ä»· + MA5 + MA10ã€åŸºç¡€ä¸Šï¼ŒåŠ å…¥ RSI ç‰¹å¾ï¼š

```python
import yfinance as yf
import pandas as pd
import numpy as np
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error
import matplotlib.pyplot as plt

# ä¸‹è½½è‹¹æœå…¬å¸è¿‘ 1 å¹´çš„å†å²æ•°æ®
data = yf.download("AAPL", period="1y")

# æ„é€ ç§»åŠ¨å¹³å‡çº¿
data["MA5"] = data["Close"].rolling(window=5).mean()
data["MA10"] = data["Close"].rolling(window=10).mean()

# æ„é€  RSI æŒ‡æ ‡
delta = data["Close"].diff()
gain = delta.where(delta > 0, 0)
loss = -delta.where(delta < 0, 0)
avg_gain = gain.rolling(window=14).mean()
avg_loss = loss.rolling(window=14).mean()
rs = avg_gain / avg_loss
data["RSI"] = 100 - (100 / (1 + rs))

# ä¸¢æ‰ NaN
data = data.dropna()

# ç‰¹å¾ = æ”¶ç›˜ä»· + MA5 + MA10 + RSI
X = data[["Close", "MA5", "MA10", "RSI"]].values
y = data["Close"].shift(-1).dropna().values
X = X[:-1]  # å¯¹é½ y

# åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†
split = int(len(X) * 0.8)
X_train, X_test = X[:split], X[split:]
y_train, y_test = y[:split], y[split:]

# å»ºæ¨¡
model = LinearRegression()
model.fit(X_train, y_train)

# é¢„æµ‹
y_pred = model.predict(X_test)

# è¯„ä¼°
mse = mean_squared_error(y_test, y_pred)
print("å‡æ–¹è¯¯å·® MSE:", mse)

# å¯è§†åŒ–
plt.figure(figsize=(12,6))
plt.plot(y_test, label="çœŸå®ä»·æ ¼")
plt.plot(y_pred, label="é¢„æµ‹ä»·æ ¼")
plt.legend()
plt.title("çº¿æ€§å›å½’ + MA + RSI é¢„æµ‹ AAPL è‚¡ä»·")
plt.show()
```

è¿è¡Œç»“æœï¼š

* æˆåŠŸè®¡ç®—äº† RSI æŒ‡æ ‡å¹¶åŠ å…¥æ¨¡å‹ã€‚
* å‡æ–¹è¯¯å·®ï¼ˆMSEï¼‰è¿›ä¸€æ­¥ä¸‹é™ï¼Œé¢„æµ‹æ›²çº¿æ¯” Day7 æ›´æ¥è¿‘çœŸå®èµ°åŠ¿ã€‚
* ç‰¹åˆ«æ˜¯åœ¨éœ‡è¡è¡Œæƒ…ä¸­ï¼ŒRSI å¸¦æ¥äº†æ›´å¥½çš„åˆ¤æ–­æ•ˆæœã€‚

---

### ğŸ“Š å®Œæˆæƒ…å†µ

* âœ… å­¦ä¹ äº† RSI æŒ‡æ ‡çš„åŸç†å’Œè®¡ç®—æ–¹æ³•
* âœ… åœ¨æ¨¡å‹ä¸­åŠ å…¥ RSI ç‰¹å¾
* âœ… æ¨¡å‹é¢„æµ‹æ•ˆæœè¿›ä¸€æ­¥æå‡
* âš ï¸ é—®é¢˜ï¼šçº¿æ€§å›å½’ä¾ç„¶å¯¹éçº¿æ€§æ³¢åŠ¨æ•æ‰æœ‰é™ï¼Œæ¨¡å‹ç²¾åº¦å­˜åœ¨ä¸Šé™

---

### ğŸ’¡ å­¦ä¹ å¿ƒå¾—

* RSI çš„å¼•å…¥è®©æˆ‘æ„Ÿå—åˆ° **åŠ¨é‡å› å­** åœ¨é‡åŒ–ä¸­çš„ä½œç”¨ã€‚
* ç‰¹å¾å·¥ç¨‹æ˜¯é¢„æµ‹æ¨¡å‹çš„çµé­‚ï¼Œæ¯”å•çº¯æ¢æ¨¡å‹æ›´é‡è¦ã€‚
* åŒæ—¶ä¹Ÿå‘ç°ï¼šè¿‡å¤šä¾èµ–çº¿æ€§å›å½’ï¼Œå¯èƒ½ä¼šé™åˆ¶æ¨¡å‹çš„è¡¨ç°ã€‚ä¸‹ä¸€æ­¥åº”è¯¥å°è¯•éçº¿æ€§æ¨¡å‹ã€‚

---

### ğŸš€ æ˜æ—¥è®¡åˆ’

* å­¦ä¹  **MACD æŒ‡æ ‡**ï¼ˆç§»åŠ¨å¹³å‡æ”¶æ•›å‘æ•£æŒ‡æ ‡ï¼‰ã€‚
* å°† MACD ä¸ RSI ç»“åˆï¼Œç»§ç»­ä¸°å¯Œç‰¹å¾å·¥ç¨‹ã€‚
* å¯¹æ¯”æ¨¡å‹åœ¨å¤šç‰¹å¾ä¸‹çš„é¢„æµ‹è¡¨ç°ã€‚

---

### ğŸ“‚ é™„å½•

* ğŸ“œ [RSI Indicator Explained](https://www.investopedia.com/terms/r/rsi.asp)
* ğŸ“œ [Relative Strength Index (RSI) in Python](https://towardsdatascience.com/relative-strength-index-rsi-in-python-a9e0c92a3c79)

---


## ğŸ“– ï¼ˆDay 9ï¼‰AIé‡åŒ–å­¦ä¹ æ—¥å¿—ï¼šMACD æŒ‡æ ‡ä¸è‚¡ä»·é¢„æµ‹

---

### ğŸ“Œ ä»Šæ—¥è®¡åˆ’

* **ä»Šæ—¥ç›®æ ‡**ï¼šå­¦ä¹  MACDï¼ˆæŒ‡æ•°å¹³æ»‘å¼‚åŒå¹³å‡çº¿ï¼‰æŒ‡æ ‡ï¼Œå¹¶ä½œä¸ºæ–°ç‰¹å¾åŠ å…¥è‚¡ä»·é¢„æµ‹æ¨¡å‹ã€‚
* **å­¦ä¹ å†…å®¹**ï¼š

  1. MACD çš„åŸºæœ¬åŸç†ä¸è®¡ç®—æ–¹æ³•
  2. Python å®ç° MACD æŒ‡æ ‡
  3. å°† MACD ç‰¹å¾åŠ å…¥å›å½’æ¨¡å‹ï¼Œä¸ Day 8 çš„ RSI ç»“åˆ

---

### ğŸ› ï¸ å­¦ä¹ è¿‡ç¨‹

#### 1. ç†è®ºå­¦ä¹ 

* **MACDï¼ˆMoving Average Convergence Divergenceï¼‰**

  * å®šä¹‰ï¼šç”± **å¿«çº¿ï¼ˆDIFï¼‰** å’Œ **æ…¢çº¿ï¼ˆDEAï¼‰** ç»„æˆçš„è¶‹åŠ¿å‹æŒ‡æ ‡ã€‚
  * è®¡ç®—æ–¹æ³•ï¼š

    1. å¿«é€Ÿ EMAï¼ˆ12 æ—¥æŒ‡æ•°ç§»åŠ¨å¹³å‡ï¼‰
    2. æ…¢é€Ÿ EMAï¼ˆ26 æ—¥æŒ‡æ•°ç§»åŠ¨å¹³å‡ï¼‰
    3. DIF = EMA(12) - EMA(26)
    4. DEA = DIF çš„ 9 æ—¥ EMA
    5. MACD = 2 Ã— (DIF - DEA)
  * è§£é‡Šï¼š

    * DIF å‘ä¸Šçªç ´ DEA â†’ ä¹°å…¥ä¿¡å·
    * DIF å‘ä¸‹çªç ´ DEA â†’ å–å‡ºä¿¡å·

* MACD ç»¼åˆäº† **è¶‹åŠ¿** å’Œ **åŠ¨é‡** çš„ç‰¹å¾ï¼Œå¸¸ä¸ RSI æ­é…ä½¿ç”¨ã€‚

---

#### 2. Python å®è·µ

åœ¨æ˜¨å¤©çš„ã€Œæ”¶ç›˜ä»· + MA + RSIã€åŸºç¡€ä¸Šï¼ŒåŠ å…¥ MACD ç‰¹å¾ï¼š

```python
import yfinance as yf
import pandas as pd
import numpy as np
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error
import matplotlib.pyplot as plt

# ä¸‹è½½è‹¹æœå…¬å¸è¿‘ 1 å¹´çš„å†å²æ•°æ®
data = yf.download("AAPL", period="1y")

# æ„é€  MA
data["MA5"] = data["Close"].rolling(window=5).mean()
data["MA10"] = data["Close"].rolling(window=10).mean()

# æ„é€  RSI
delta = data["Close"].diff()
gain = delta.where(delta > 0, 0)
loss = -delta.where(delta < 0, 0)
avg_gain = gain.rolling(window=14).mean()
avg_loss = loss.rolling(window=14).mean()
rs = avg_gain / avg_loss
data["RSI"] = 100 - (100 / (1 + rs))

# æ„é€  MACD
ema12 = data["Close"].ewm(span=12, adjust=False).mean()
ema26 = data["Close"].ewm(span=26, adjust=False).mean()
data["DIF"] = ema12 - ema26
data["DEA"] = data["DIF"].ewm(span=9, adjust=False).mean()
data["MACD"] = 2 * (data["DIF"] - data["DEA"])

# ä¸¢æ‰ NaN
data = data.dropna()

# ç‰¹å¾ = æ”¶ç›˜ä»· + MA5 + MA10 + RSI + MACD
X = data[["Close", "MA5", "MA10", "RSI", "MACD"]].values
y = data["Close"].shift(-1).dropna().values
X = X[:-1]  # å¯¹é½ y

# åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†
split = int(len(X) * 0.8)
X_train, X_test = X[:split], X[split:]
y_train, y_test = y[:split], y[split:]

# å»ºæ¨¡
model = LinearRegression()
model.fit(X_train, y_train)

# é¢„æµ‹
y_pred = model.predict(X_test)

# è¯„ä¼°
mse = mean_squared_error(y_test, y_pred)
print("å‡æ–¹è¯¯å·® MSE:", mse)

# å¯è§†åŒ–
plt.figure(figsize=(12,6))
plt.plot(y_test, label="çœŸå®ä»·æ ¼")
plt.plot(y_pred, label="é¢„æµ‹ä»·æ ¼")
plt.legend()
plt.title("çº¿æ€§å›å½’ + MA + RSI + MACD é¢„æµ‹ AAPL è‚¡ä»·")
plt.show()
```

è¿è¡Œç»“æœï¼š

* æˆåŠŸè®¡ç®—äº† MACD æŒ‡æ ‡ï¼Œå¹¶ä¸ RSI ä¸€èµ·åŠ å…¥æ¨¡å‹ã€‚
* é¢„æµ‹æ›²çº¿è¿›ä¸€æ­¥è´´è¿‘çœŸå®èµ°åŠ¿ï¼Œå°¤å…¶åœ¨è¶‹åŠ¿è½¬æ¢åŒºé—´æ›´æ•æ„Ÿã€‚
* å‡æ–¹è¯¯å·®ï¼ˆMSEï¼‰è¿›ä¸€æ­¥ä¸‹é™ï¼Œè¯´æ˜æ¨¡å‹çš„ç‰¹å¾è¡¨è¾¾èƒ½åŠ›å¢å¼ºã€‚

---

### ğŸ“Š å®Œæˆæƒ…å†µ

* âœ… å­¦ä¹ äº† MACD çš„è®¡ç®—æ–¹æ³•å’Œäº¤æ˜“ä¿¡å·
* âœ… åœ¨æ¨¡å‹ä¸­åŠ å…¥ MACD ç‰¹å¾
* âœ… æ¨¡å‹æ•ˆæœè¾ƒ Day 8 å†æ¬¡æå‡
* âš ï¸ é—®é¢˜ï¼šè™½ç„¶é¢„æµ‹æ›´å‡†ï¼Œä½†çº¿æ€§å›å½’å¯¹éçº¿æ€§å…³ç³»çš„æ‹Ÿåˆèƒ½åŠ›ä»æœ‰é™

---

### ğŸ’¡ å­¦ä¹ å¿ƒå¾—

* ä»Šå¤©å°è¯•å°† **è¶‹åŠ¿æŒ‡æ ‡ï¼ˆMAã€MACDï¼‰** å’Œ **åŠ¨é‡æŒ‡æ ‡ï¼ˆRSIï¼‰** ç»“åˆï¼Œæ„Ÿè§‰æ¨¡å‹æ›´â€œèªæ˜â€äº†ã€‚
* æ·±åˆ»ä½“ä¼šåˆ°ï¼š

  * **å•ä¸€å› å­ä¸è¶³ä»¥è§£é‡Šå¸‚åœº**
  * **å¤šå› å­ç»„åˆèƒ½æå‡æ¨¡å‹è¡¨ç°**
* ä½†æ˜¯ï¼Œéšç€ç‰¹å¾è¶Šæ¥è¶Šå¤æ‚ï¼Œçº¿æ€§å›å½’çš„å±€é™æ€§è¶Šæ¥è¶Šæ˜æ˜¾ï¼Œä¸‹ä¸€æ­¥éœ€è¦æ›´å¼ºçš„æœºå™¨å­¦ä¹ æ¨¡å‹æ¥å¤„ç†ã€‚

---

### ğŸš€ æ˜æ—¥è®¡åˆ’

* å­¦ä¹  **éšæœºæ£®æ—å›å½’ï¼ˆRandom Forest Regressorï¼‰**
* ç”¨éšæœºæ£®æ—æ›¿ä»£çº¿æ€§å›å½’ï¼Œæ¯”è¾ƒæ¨¡å‹æ•ˆæœ
* ä½“éªŒéçº¿æ€§æ¨¡å‹åœ¨è‚¡ä»·é¢„æµ‹ä¸­çš„ä¼˜åŠ¿

---

### ğŸ“‚ é™„å½•

* ğŸ“œ [MACD Indicator Explained](https://www.investopedia.com/terms/m/macd.asp)
* ğŸ“œ [Python å®ç° MACD](https://www.tradingview.com/support/solutions/43000502338-macd/)

---

## ğŸ“– ï¼ˆDay 10ï¼‰AIé‡åŒ–å­¦ä¹ æ—¥å¿—ï¼šéšæœºæ£®æ—é¢„æµ‹è‚¡ä»·

---

### ğŸ“Œ ä»Šæ—¥è®¡åˆ’

* **ä»Šæ—¥ç›®æ ‡**ï¼šå­¦ä¹ å¹¶åº”ç”¨ **éšæœºæ£®æ—å›å½’ï¼ˆRandom Forest Regressorï¼‰** æ¥é¢„æµ‹è‚¡ä»·ï¼Œå¹¶ä¸å‰é¢ç”¨åˆ°çš„çº¿æ€§å›å½’è¿›è¡Œæ•ˆæœå¯¹æ¯”ã€‚
* **å­¦ä¹ å†…å®¹**ï¼š

  1. éšæœºæ£®æ—çš„åŸºæœ¬åŸç†
  2. ç”¨ Python å®ç°éšæœºæ£®æ—é¢„æµ‹è‚¡ä»·
  3. ä¸çº¿æ€§å›å½’çš„è¡¨ç°è¿›è¡Œå¯¹æ¯”

---

### ğŸ› ï¸ å­¦ä¹ è¿‡ç¨‹

#### 1. ç†è®ºå­¦ä¹ 

* **éšæœºæ£®æ—ï¼ˆRandom Forestï¼‰**

  * æ˜¯ç”± **å¤šæ£µå†³ç­–æ ‘** ç»„æˆçš„é›†æˆå­¦ä¹ æ¨¡å‹
  * æ¯æ£µæ ‘åœ¨æ•°æ®å­é›†ä¸Šè®­ç»ƒï¼Œæœ€åå–å¹³å‡é¢„æµ‹ç»“æœï¼ˆå›å½’åœºæ™¯ï¼‰
  * ä¼˜ç‚¹ï¼š

    * èƒ½å¤„ç† **éçº¿æ€§å…³ç³»**
    * å¯¹å¼‚å¸¸å€¼ **é²æ£’æ€§å¼º**
    * ç‰¹å¾é‡è¦æ€§å¯è§£é‡Š
  * ç¼ºç‚¹ï¼š

    * å¯è§£é‡Šæ€§ä¸å¦‚çº¿æ€§å›å½’
    * æ¨¡å‹è¾ƒå¤§ï¼Œè®¡ç®—å¼€é”€æ›´é«˜

ç›¸æ¯”çº¿æ€§å›å½’ï¼Œéšæœºæ£®æ—æ›´é€‚åˆè‚¡ä»·è¿™ç§ **å¤æ‚éçº¿æ€§æ•°æ®**ã€‚

---

#### 2. Python å®è·µ

åœ¨æ˜¨å¤©æ„é€ çš„ç‰¹å¾ï¼ˆæ”¶ç›˜ä»· + MA + RSI + MACDï¼‰çš„åŸºç¡€ä¸Šï¼Œå¼•å…¥éšæœºæ£®æ—æ¨¡å‹ã€‚

```python
import yfinance as yf
import pandas as pd
import numpy as np
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error
import matplotlib.pyplot as plt

# ä¸‹è½½æ•°æ®
data = yf.download("AAPL", period="1y")

# æ„é€  MA
data["MA5"] = data["Close"].rolling(window=5).mean()
data["MA10"] = data["Close"].rolling(window=10).mean()

# æ„é€  RSI
delta = data["Close"].diff()
gain = delta.where(delta > 0, 0)
loss = -delta.where(delta < 0, 0)
avg_gain = gain.rolling(window=14).mean()
avg_loss = loss.rolling(window=14).mean()
rs = avg_gain / avg_loss
data["RSI"] = 100 - (100 / (1 + rs))

# æ„é€  MACD
ema12 = data["Close"].ewm(span=12, adjust=False).mean()
ema26 = data["Close"].ewm(span=26, adjust=False).mean()
data["DIF"] = ema12 - ema26
data["DEA"] = data["DIF"].ewm(span=9, adjust=False).mean()
data["MACD"] = 2 * (data["DIF"] - data["DEA"])

# ä¸¢æ‰ NaN
data = data.dropna()

# ç‰¹å¾ & æ ‡ç­¾
X = data[["Close", "MA5", "MA10", "RSI", "MACD"]].values
y = data["Close"].shift(-1).dropna().values
X = X[:-1]

# åˆ’åˆ†è®­ç»ƒ & æµ‹è¯•
split = int(len(X) * 0.8)
X_train, X_test = X[:split], X[split:]
y_train, y_test = y[:split], y[split:]

# çº¿æ€§å›å½’
lr = LinearRegression()
lr.fit(X_train, y_train)
y_pred_lr = lr.predict(X_test)

# éšæœºæ£®æ—
rf = RandomForestRegressor(n_estimators=200, random_state=42)
rf.fit(X_train, y_train)
y_pred_rf = rf.predict(X_test)

# è¯„ä¼°
mse_lr = mean_squared_error(y_test, y_pred_lr)
mse_rf = mean_squared_error(y_test, y_pred_rf)

print("çº¿æ€§å›å½’ MSE:", mse_lr)
print("éšæœºæ£®æ— MSE:", mse_rf)

# å¯è§†åŒ–å¯¹æ¯”
plt.figure(figsize=(12,6))
plt.plot(y_test, label="çœŸå®ä»·æ ¼", color="black")
plt.plot(y_pred_lr, label="çº¿æ€§å›å½’é¢„æµ‹", linestyle="--")
plt.plot(y_pred_rf, label="éšæœºæ£®æ—é¢„æµ‹", linestyle="-.")
plt.legend()
plt.title("çº¿æ€§å›å½’ vs éšæœºæ£®æ— è‚¡ä»·é¢„æµ‹")
plt.show()
```

**è¿è¡Œç»“æœï¼ˆç¤ºä¾‹ï¼‰**ï¼š

* çº¿æ€§å›å½’ MSEï¼šâ‰ˆ 5.8
* éšæœºæ£®æ— MSEï¼šâ‰ˆ 2.9
* éšæœºæ£®æ—çš„æ‹Ÿåˆæ•ˆæœæ˜æ˜¾æ›´å¥½ï¼Œé¢„æµ‹æ›²çº¿æ›´è´´è¿‘çœŸå®èµ°åŠ¿ï¼Œå°¤å…¶åœ¨éœ‡è¡è¡Œæƒ…ä¸‹è¡¨ç°ä¼˜äºçº¿æ€§å›å½’ã€‚

---

### ğŸ“Š å®Œæˆæƒ…å†µ

* âœ… å­¦ä¹ å¹¶æŒæ¡äº†éšæœºæ£®æ—å›å½’çš„åŸºæœ¬åŸç†
* âœ… åœ¨è‚¡ä»·é¢„æµ‹ä¸­æˆåŠŸåº”ç”¨éšæœºæ£®æ—
* âœ… ä¸çº¿æ€§å›å½’è¿›è¡Œå¯¹æ¯”ï¼Œæ•ˆæœæ˜¾è‘—æå‡
* âš ï¸ éœ€è¦æ³¨æ„é¿å…è¿‡æ‹Ÿåˆï¼ˆæ¯”å¦‚è°ƒèŠ‚ `max_depth`ã€`min_samples_split` å‚æ•°ï¼‰

---

### ğŸ’¡ å­¦ä¹ å¿ƒå¾—

* ä»Šå¤©ç¬¬ä¸€æ¬¡å¼•å…¥ **éçº¿æ€§æœºå™¨å­¦ä¹ æ¨¡å‹**ï¼Œæ•ˆæœç«‹ç«¿è§å½±ã€‚
* éšæœºæ£®æ—è®©æ¨¡å‹â€œçœ‹æ‡‚â€äº†è‚¡ä»·é‡Œçš„å¤æ‚æ¨¡å¼ï¼Œè€Œä¸ä»…ä»…æ˜¯ç®€å•çš„ç›´çº¿æ‹Ÿåˆã€‚
* æœªæ¥é¢„æµ‹ä¸­ï¼Œå¯èƒ½ä¼šå°è¯•æ›´å¤š **é›†æˆå­¦ä¹ æ–¹æ³•**ï¼ˆæ¯”å¦‚æ¢¯åº¦æå‡ã€XGBoostï¼‰ã€‚

---

### ğŸš€ æ˜æ—¥è®¡åˆ’

* å­¦ä¹  **ç‰¹å¾é‡è¦æ€§ï¼ˆFeature Importanceï¼‰** åˆ†æ
* äº†è§£å“ªäº›å› å­ï¼ˆMAã€RSIã€MACDã€æ”¶ç›˜ä»·ï¼‰å¯¹è‚¡ä»·é¢„æµ‹è´¡çŒ®æœ€å¤§
* ä¸ºåç»­çš„ã€Œå› å­ç­›é€‰ã€æ‰“ä¸‹åŸºç¡€

---

### ğŸ“‚ é™„å½•

* ğŸ“œ [éšæœºæ£®æ—åŸç† - Scikit-learn å®˜æ–¹æ–‡æ¡£](https://scikit-learn.org/stable/modules/ensemble.html#forest)
* ğŸ“œ [è‚¡ä»·é¢„æµ‹ä¸­çš„éšæœºæ£®æ—åº”ç”¨](https://towardsdatascience.com/random-forest-in-machine-learning-641b9c4e8052)

---


## ğŸ“– ï¼ˆDay 11ï¼‰AIé‡åŒ–å­¦ä¹ æ—¥å¿—ï¼šç‰¹å¾é‡è¦æ€§åˆ†æ

---

### ğŸ“Œ ä»Šæ—¥è®¡åˆ’

* **ä»Šæ—¥ç›®æ ‡**ï¼šåˆ©ç”¨éšæœºæ£®æ—æ¨¡å‹åˆ†æç‰¹å¾çš„é‡è¦æ€§ï¼Œæ‰¾å‡ºå¯¹è‚¡ä»·é¢„æµ‹å½±å“æœ€å¤§çš„æŒ‡æ ‡ã€‚
* **å­¦ä¹ å†…å®¹**ï¼š

  1. ç‰¹å¾é‡è¦æ€§çš„æ¦‚å¿µ
  2. åœ¨è‚¡ä»·é¢„æµ‹ä¸­çš„åº”ç”¨
  3. Python å®ç°å¹¶å¯è§†åŒ–ç‰¹å¾é‡è¦æ€§

---

### ğŸ› ï¸ å­¦ä¹ è¿‡ç¨‹

#### 1. ç†è®ºå­¦ä¹ 

* **ç‰¹å¾é‡è¦æ€§ï¼ˆFeature Importanceï¼‰**

  * è¡¡é‡æ¨¡å‹ä¸­æ¯ä¸ªç‰¹å¾å¯¹é¢„æµ‹ç»“æœçš„è´¡çŒ®åº¦ã€‚
  * åœ¨ **éšæœºæ£®æ—** ä¸­ï¼Œé€šå¸¸ç”¨â€œç‰¹å¾åˆ†è£‚æ—¶å¸¦æ¥çš„ä¿¡æ¯å¢ç›Šâ€æ¥è¡¡é‡ã€‚
  * å¥½å¤„ï¼š

    * èƒ½å¸®åŠ©æˆ‘ä»¬ç†è§£æ¨¡å‹çš„â€œå†³ç­–ä¾æ®â€ã€‚
    * å¯ä»¥ç­›é€‰æ‰è´¡çŒ®åº¦ä½çš„å› å­ï¼Œé¿å…è¿‡æ‹Ÿåˆã€‚

åœ¨é‡åŒ–æŠ•èµ„ä¸­ï¼Œç‰¹å¾é‡è¦æ€§åˆ†æå¸¸ç”¨äº **å› å­ç­›é€‰**ï¼šä¿ç•™é«˜è´¡çŒ®çš„å› å­ï¼Œå‰”é™¤æ— å…³æˆ–å™ªå£°å› å­ã€‚

---

#### 2. Python å®è·µ

æ²¿ç”¨æ˜¨å¤©çš„ `éšæœºæ£®æ—æ¨¡å‹`ï¼Œå¢åŠ ç‰¹å¾é‡è¦æ€§åˆ†æï¼š

```python
import yfinance as yf
import pandas as pd
import numpy as np
from sklearn.ensemble import RandomForestRegressor
import matplotlib.pyplot as plt

# ä¸‹è½½æ•°æ®
data = yf.download("AAPL", period="1y")

# æ„é€  MA
data["MA5"] = data["Close"].rolling(window=5).mean()
data["MA10"] = data["Close"].rolling(window=10).mean()

# æ„é€  RSI
delta = data["Close"].diff()
gain = delta.where(delta > 0, 0)
loss = -delta.where(delta < 0, 0)
avg_gain = gain.rolling(window=14).mean()
avg_loss = loss.rolling(window=14).mean()
rs = avg_gain / avg_loss
data["RSI"] = 100 - (100 / (1 + rs))

# æ„é€  MACD
ema12 = data["Close"].ewm(span=12, adjust=False).mean()
ema26 = data["Close"].ewm(span=26, adjust=False).mean()
data["DIF"] = ema12 - ema26
data["DEA"] = data["DIF"].ewm(span=9, adjust=False).mean()
data["MACD"] = 2 * (data["DIF"] - data["DEA"])

# ä¸¢æ‰ NaN
data = data.dropna()

# ç‰¹å¾ & æ ‡ç­¾
X = data[["Close", "MA5", "MA10", "RSI", "MACD"]].values
y = data["Close"].shift(-1).dropna().values
X = X[:-1]

# è®­ç»ƒé›† & æµ‹è¯•é›†
split = int(len(X) * 0.8)
X_train, X_test = X[:split], X[split:]
y_train, y_test = y[:split], y[split:]

# éšæœºæ£®æ—
rf = RandomForestRegressor(n_estimators=200, random_state=42)
rf.fit(X_train, y_train)

# ç‰¹å¾é‡è¦æ€§
importances = rf.feature_importances_
features = ["Close", "MA5", "MA10", "RSI", "MACD"]

# å¯è§†åŒ–
plt.figure(figsize=(8,5))
plt.barh(features, importances, color="skyblue")
plt.title("ç‰¹å¾é‡è¦æ€§åˆ†æ - éšæœºæ£®æ—")
plt.xlabel("Importance")
plt.show()
```

**è¿è¡Œç»“æœï¼ˆç¤ºä¾‹ï¼‰**ï¼š

* ç‰¹å¾é‡è¦æ€§æ’åºå¤§æ¦‚æ˜¯ï¼š

  1. **Closeï¼ˆæ”¶ç›˜ä»·ï¼‰**ï¼šæœ€é«˜ï¼Œè¯´æ˜ä»·æ ¼æœ¬èº«æ˜¯æœªæ¥èµ°åŠ¿çš„æœ€å¼ºä¿¡å·
  2. **MA5 / MA10ï¼ˆå‡çº¿ï¼‰**ï¼šæ¬¡ä¹‹ï¼Œè¶‹åŠ¿è·Ÿéšçš„å› å­æœ‰æ•ˆ
  3. **MACD**ï¼šä¸€å®šç¨‹åº¦æœ‰æ•ˆ
  4. **RSI**ï¼šåœ¨è‹¹æœè¿‡å» 1 å¹´æ•°æ®é‡Œå½±å“ç›¸å¯¹è¾ƒå¼±

---

### ğŸ“Š å®Œæˆæƒ…å†µ

* âœ… æŒæ¡äº†ç‰¹å¾é‡è¦æ€§åˆ†æçš„æ¦‚å¿µ
* âœ… åˆ©ç”¨éšæœºæ£®æ—å®ç°äº†è‚¡ä»·é¢„æµ‹å› å­çš„é‡è¦æ€§æ’åº
* âœ… å‘ç°æ”¶ç›˜ä»·ã€å‡çº¿æ˜¯å¯¹è‚¡ä»·é¢„æµ‹æœ€æœ‰æ•ˆçš„å› å­
* âš ï¸ æ³¨æ„ï¼šç‰¹å¾é‡è¦æ€§ç»“æœä¾èµ–äºæ ·æœ¬å’Œæ—¶é—´æ®µï¼Œä¸ä»£è¡¨æ°¸ä¹…æœ‰æ•ˆ

---

### ğŸ’¡ å­¦ä¹ å¿ƒå¾—

* ä»Šå¤©çš„ç»“æœè®©æˆ‘è®¤è¯†åˆ°ï¼š**ä¸æ˜¯æ‰€æœ‰å› å­éƒ½å¯¹é¢„æµ‹æœ‰å¸®åŠ©**ã€‚
* åœ¨é‡‘èå¸‚åœºä¸­ï¼Œå¾ˆå¤šâ€œå¤æ‚æŒ‡æ ‡â€å…¶å®å¯èƒ½ä¸å¦‚ **ä»·æ ¼æœ¬èº« + ç®€å•å‡çº¿** æœ‰ç”¨ã€‚
* ä¸‹ä¸€æ­¥ï¼Œå¯ä»¥å°è¯•ï¼š**åŠ å…¥æ›´å¤šå®è§‚æ•°æ®æˆ–æˆäº¤é‡å› å­**ï¼Œå†åšç‰¹å¾é‡è¦æ€§åˆ†æï¼Œçœ‹çœ‹ä¼šä¸ä¼šæœ‰æ–°çš„å‘ç°ã€‚

---

### ğŸš€ æ˜æ—¥è®¡åˆ’

* å­¦ä¹  **è¶…å‚æ•°è°ƒä¼˜ï¼ˆHyperparameter Tuningï¼‰**
* ä½¿ç”¨ `GridSearchCV` æˆ– `RandomizedSearchCV` ä¼˜åŒ–éšæœºæ£®æ—çš„å‚æ•°
* è§‚å¯Ÿè°ƒä¼˜å‰åçš„é¢„æµ‹æ•ˆæœå·®å¼‚

---

### ğŸ“‚ é™„å½•

* ğŸ“œ [Scikit-learn - Feature Importance](https://scikit-learn.org/stable/auto_examples/ensemble/plot_forest_importances.html)
* ğŸ“œ [é‡åŒ–æŠ•èµ„ä¸­çš„å› å­é€‰æ‹©](https://towardsdatascience.com/feature-selection-techniques-in-machine-learning-with-python-f24e7da3f36e)

---

## ğŸ“– ï¼ˆDay 12ï¼‰AIé‡åŒ–å­¦ä¹ æ—¥å¿—ï¼šéšæœºæ£®æ—è¶…å‚æ•°è°ƒä¼˜

---

### ğŸ“Œ ä»Šæ—¥è®¡åˆ’

* **ä»Šæ—¥ç›®æ ‡**ï¼šå­¦ä¹ å¦‚ä½•é€šè¿‡è°ƒå‚ä¼˜åŒ–éšæœºæ£®æ—æ¨¡å‹ï¼Œè®©è‚¡ä»·é¢„æµ‹æ•ˆæœæ›´å¥½ã€‚
* **å­¦ä¹ å†…å®¹**ï¼š

  1. éšæœºæ£®æ—çš„ä¸»è¦è¶…å‚æ•°
  2. ç”¨ `GridSearchCV` è‡ªåŠ¨è°ƒå‚
  3. å¯¹æ¯”è°ƒå‚å‰åçš„é¢„æµ‹æ•ˆæœ

---

### ğŸ› ï¸ å­¦ä¹ è¿‡ç¨‹

#### 1. ç†è®ºå­¦ä¹ 

éšæœºæ£®æ—çš„å¸¸è§è¶…å‚æ•°ï¼š

* **n\_estimators**ï¼šæ ‘çš„æ•°é‡ï¼ˆè¶Šå¤šè¶Šç¨³å®šï¼Œä½†è®¡ç®—æ›´æ…¢ï¼‰
* **max\_depth**ï¼šæ ‘çš„æœ€å¤§æ·±åº¦ï¼ˆé™åˆ¶è¿‡æ‹Ÿåˆï¼‰
* **min\_samples\_split**ï¼šä¸€ä¸ªèŠ‚ç‚¹è‡³å°‘è¦æœ‰å¤šå°‘æ ·æœ¬æ‰èƒ½ç»§ç»­åˆ†è£‚
* **min\_samples\_leaf**ï¼šå¶å­èŠ‚ç‚¹æœ€å°‘æ ·æœ¬æ•°
* **max\_features**ï¼šæ¯æ¬¡åˆ†è£‚æ—¶è€ƒè™‘çš„ç‰¹å¾æ•°

ğŸ‘‰ è°ƒå‚çš„ç›®æ ‡ï¼š

* **é™ä½è¿‡æ‹Ÿåˆ**ï¼ˆé˜²æ­¢æ¨¡å‹åªè®°ä½å†å²ä»·æ ¼è€Œç¼ºä¹æ³›åŒ–èƒ½åŠ›ï¼‰
* **æé«˜é¢„æµ‹ç²¾åº¦**ï¼ˆåœ¨æµ‹è¯•é›†ä¸Šè¡¨ç°æ›´å¥½ï¼‰

---

#### 2. Python å®è·µ

è¿™é‡Œç”¨ `GridSearchCV` æœç´¢æœ€ä¼˜å‚æ•°ç»„åˆï¼š

```python
import yfinance as yf
import pandas as pd
import numpy as np
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import GridSearchCV, train_test_split
from sklearn.metrics import mean_squared_error
import matplotlib.pyplot as plt

# ä¸‹è½½æ•°æ®
data = yf.download("AAPL", period="1y")

# æ„é€ ç‰¹å¾ï¼ˆå‡çº¿ã€RSIã€MACDï¼‰
data["MA5"] = data["Close"].rolling(window=5).mean()
data["MA10"] = data["Close"].rolling(window=10).mean()

delta = data["Close"].diff()
gain = delta.where(delta > 0, 0)
loss = -delta.where(delta < 0, 0)
avg_gain = gain.rolling(window=14).mean()
avg_loss = loss.rolling(window=14).mean()
rs = avg_gain / avg_loss
data["RSI"] = 100 - (100 / (1 + rs))

ema12 = data["Close"].ewm(span=12, adjust=False).mean()
ema26 = data["Close"].ewm(span=26, adjust=False).mean()
data["DIF"] = ema12 - ema26
data["DEA"] = data["DIF"].ewm(span=9, adjust=False).mean()
data["MACD"] = 2 * (data["DIF"] - data["DEA"])

data = data.dropna()

X = data[["Close", "MA5", "MA10", "RSI", "MACD"]].values
y = data["Close"].shift(-1).dropna().values
X = X[:-1]

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)

# è°ƒå‚å‰æ¨¡å‹
rf_default = RandomForestRegressor(random_state=42)
rf_default.fit(X_train, y_train)
y_pred_default = rf_default.predict(X_test)
mse_default = mean_squared_error(y_test, y_pred_default)

# GridSearchCV è°ƒå‚
param_grid = {
    "n_estimators": [100, 200, 300],
    "max_depth": [5, 10, 15, None],
    "min_samples_split": [2, 5, 10],
    "min_samples_leaf": [1, 2, 4],
    "max_features": ["sqrt", "log2"]
}

grid_search = GridSearchCV(
    estimator=RandomForestRegressor(random_state=42),
    param_grid=param_grid,
    cv=3,
    n_jobs=-1,
    scoring="neg_mean_squared_error"
)

grid_search.fit(X_train, y_train)
best_model = grid_search.best_estimator_

# è°ƒå‚åæ¨¡å‹é¢„æµ‹
y_pred_best = best_model.predict(X_test)
mse_best = mean_squared_error(y_test, y_pred_best)

print("é»˜è®¤å‚æ•° MSE:", mse_default)
print("æœ€ä½³å‚æ•°ç»„åˆ:", grid_search.best_params_)
print("è°ƒå‚å MSE:", mse_best)

# å¯è§†åŒ–
plt.figure(figsize=(12,6))
plt.plot(y_test, label="çœŸå®ä»·æ ¼", color="black")
plt.plot(y_pred_default, label="é»˜è®¤å‚æ•°é¢„æµ‹", linestyle="--")
plt.plot(y_pred_best, label="è°ƒå‚åé¢„æµ‹", linestyle="-.")
plt.legend()
plt.title("éšæœºæ£®æ—è°ƒå‚å‰ vs è°ƒå‚å è‚¡ä»·é¢„æµ‹")
plt.show()
```

**è¿è¡Œç»“æœï¼ˆç¤ºä¾‹ï¼‰**ï¼š

* é»˜è®¤å‚æ•° MSEï¼šâ‰ˆ 3.5
* è°ƒå‚å MSEï¼šâ‰ˆ 2.3
* æœ€ä½³å‚æ•°ç»„åˆï¼š

  ```python
  {
      'n_estimators': 200,
      'max_depth': 10,
      'min_samples_split': 5,
      'min_samples_leaf': 2,
      'max_features': 'sqrt'
  }
  ```
* è°ƒå‚åé¢„æµ‹æ›²çº¿æ›´è´´è¿‘çœŸå®ä»·æ ¼ï¼Œéœ‡è¡ç‚¹ä¹Ÿæ›´å‡†ç¡®ã€‚

---

### ğŸ“Š å®Œæˆæƒ…å†µ

* âœ… å­¦ä¹ äº†éšæœºæ£®æ—çš„ä¸»è¦è¶…å‚æ•°åŠå…¶ä½œç”¨
* âœ… ä½¿ç”¨ GridSearchCV æˆåŠŸæ‰¾åˆ°æœ€ä½³å‚æ•°ç»„åˆ
* âœ… è°ƒå‚åé¢„æµ‹æ•ˆæœæ˜¾è‘—æå‡
* âš ï¸ æ³¨æ„ï¼šè°ƒå‚è€—æ—¶è¾ƒé•¿ï¼Œå°¤å…¶æ•°æ®é‡å¤§æ—¶è¦è€ƒè™‘æ•ˆç‡ï¼ˆå¯ä»¥ç”¨ `RandomizedSearchCV` æ›¿ä»£ï¼‰

---

### ğŸ’¡ å­¦ä¹ å¿ƒå¾—

* ä»Šå¤©ä½“éªŒäº† **è‡ªåŠ¨åŒ–è°ƒå‚**ï¼Œæ„Ÿå—åˆ° AI åœ¨é‡åŒ–ä¸­çš„â€œé»‘ç®±â€ä¸€é¢ã€‚
* è°ƒå‚åæ•ˆæœæå‡è®©æˆ‘æ„è¯†åˆ°ï¼š**åŒä¸€ä¸ªæ¨¡å‹ï¼Œè°ƒå‚å‰åå·®è·å¯èƒ½éå¸¸å¤§**ã€‚
* ä»Šåè¦è€ƒè™‘ **å‚æ•°è°ƒä¼˜ + ç‰¹å¾å·¥ç¨‹** ç»“åˆï¼Œæ‰èƒ½è·å¾—æ›´ç¨³å®šçš„é¢„æµ‹æ•ˆæœã€‚

---

### ğŸš€ æ˜æ—¥è®¡åˆ’

* å­¦ä¹  **æ—¶é—´åºåˆ—äº¤å‰éªŒè¯ï¼ˆTimeSeriesSplitï¼‰**
* æ”¹è¿›æ¨¡å‹è¯„ä¼°æ–¹å¼ï¼Œè®©éªŒè¯æ›´è´´è¿‘çœŸå®äº¤æ˜“åœºæ™¯
* å°è¯•åœ¨è‚¡ä»·é¢„æµ‹ä¸­åº”ç”¨æ—¶é—´åºåˆ—äº¤å‰éªŒè¯

---

### ğŸ“‚ é™„å½•

* ğŸ“œ [Scikit-learn GridSearchCV æ–‡æ¡£](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html)
* ğŸ“œ [Random Forest å‚æ•°è°ƒä¼˜ç»éªŒ](https://towardsdatascience.com/random-forest-hyperparameters-and-how-to-tune-them-17b3723aebeb)

---

## ğŸ“– ï¼ˆDay 13ï¼‰AIé‡åŒ–å­¦ä¹ æ—¥å¿—ï¼šæ—¶é—´åºåˆ—äº¤å‰éªŒè¯

---

### ğŸ“Œ ä»Šæ—¥è®¡åˆ’

* **ä»Šæ—¥ç›®æ ‡**ï¼šæŒæ¡æ—¶é—´åºåˆ—äº¤å‰éªŒè¯æ–¹æ³•ï¼Œå¹¶å°†å…¶åº”ç”¨åˆ°è‚¡ä»·é¢„æµ‹ä¸­ã€‚
* **å­¦ä¹ å†…å®¹**ï¼š

  1. ä¸ºä»€ä¹ˆä¸èƒ½ç”¨æ™®é€šäº¤å‰éªŒè¯è¯„ä¼°è‚¡ä»·é¢„æµ‹
  2. æ—¶é—´åºåˆ—äº¤å‰éªŒè¯çš„åŸç†
  3. Python å®ç° `TimeSeriesSplit` è¯„ä¼°éšæœºæ£®æ—æ¨¡å‹

---

### ğŸ› ï¸ å­¦ä¹ è¿‡ç¨‹

#### 1. ç†è®ºå­¦ä¹ 

åœ¨è‚¡ç¥¨é¢„æµ‹ä¸­ï¼š

* æ•°æ®æœ‰ **æ—¶é—´å…ˆåé¡ºåº**ï¼Œæœªæ¥æ•°æ®ä¸èƒ½ç”¨æ¥é¢„æµ‹è¿‡å»ã€‚
* å¦‚æœç”¨æ™®é€šçš„ `KFold` éšæœºåˆ’åˆ†ï¼Œä¼šé€ æˆ **ä¿¡æ¯æ³„æ¼**ï¼ˆæœªæ¥æ•°æ®è¿›å…¥è®­ç»ƒé›†ï¼‰ã€‚

ğŸ‘‰ è§£å†³åŠæ³•ï¼š

* **æ—¶é—´åºåˆ—äº¤å‰éªŒè¯ï¼ˆTimeSeriesSplitï¼‰**

  * æ¯æ¬¡è®­ç»ƒé›†ä½¿ç”¨è¿‡å»çš„æ•°æ®
  * æµ‹è¯•é›†ä½¿ç”¨ç´§éšå…¶åçš„æœªæ¥æ•°æ®
  * æ›´ç¬¦åˆå®ç›˜é€»è¾‘

ç¤ºæ„å›¾ï¼ˆ5 æŠ˜ TSCVï¼‰ï¼š

```
Split 1: è®­ç»ƒ [1]    æµ‹è¯• [2]
Split 2: è®­ç»ƒ [1 2]  æµ‹è¯• [3]
Split 3: è®­ç»ƒ [1 2 3] æµ‹è¯• [4]
...
```

---

#### 2. Python å®è·µ

```python
import yfinance as yf
import pandas as pd
import numpy as np
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import TimeSeriesSplit
from sklearn.metrics import mean_squared_error
import matplotlib.pyplot as plt

# ä¸‹è½½æ•°æ®
data = yf.download("AAPL", period="1y")

# æ„é€ ç‰¹å¾
data["MA5"] = data["Close"].rolling(window=5).mean()
data["MA10"] = data["Close"].rolling(window=10).mean()

delta = data["Close"].diff()
gain = delta.where(delta > 0, 0)
loss = -delta.where(delta < 0, 0)
avg_gain = gain.rolling(window=14).mean()
avg_loss = loss.rolling(window=14).mean()
rs = avg_gain / avg_loss
data["RSI"] = 100 - (100 / (1 + rs))

ema12 = data["Close"].ewm(span=12, adjust=False).mean()
ema26 = data["Close"].ewm(span=26, adjust=False).mean()
data["DIF"] = ema12 - ema26
data["DEA"] = data["DIF"].ewm(span=9, adjust=False).mean()
data["MACD"] = 2 * (data["DIF"] - data["DEA"])

data = data.dropna()

X = data[["Close", "MA5", "MA10", "RSI", "MACD"]].values
y = data["Close"].shift(-1).dropna().values
X = X[:-1]

# æ—¶é—´åºåˆ—äº¤å‰éªŒè¯
tscv = TimeSeriesSplit(n_splits=5)
rf = RandomForestRegressor(n_estimators=200, max_depth=10, random_state=42)

mse_scores = []
fold = 1

for train_idx, test_idx in tscv.split(X):
    X_train, X_test = X[train_idx], X[test_idx]
    y_train, y_test = y[train_idx], y[test_idx]
    
    rf.fit(X_train, y_train)
    y_pred = rf.predict(X_test)
    
    mse = mean_squared_error(y_test, y_pred)
    mse_scores.append(mse)
    print(f"Fold {fold} MSE: {mse:.4f}")
    fold += 1

print("å¹³å‡ MSE:", np.mean(mse_scores))

# å¯è§†åŒ–æœ€åä¸€æŠ˜é¢„æµ‹
plt.figure(figsize=(12,6))
plt.plot(y_test, label="çœŸå®ä»·æ ¼", color="black")
plt.plot(y_pred, label="é¢„æµ‹ä»·æ ¼", linestyle="--")
plt.legend()
plt.title("TimeSeriesSplit æœ€åä¸€æŠ˜é¢„æµ‹æ•ˆæœ")
plt.show()
```

**è¿è¡Œç»“æœï¼ˆç¤ºä¾‹ï¼‰**ï¼š

* Fold 1 MSE: 3.25
* Fold 2 MSE: 2.90
* Fold 3 MSE: 2.75
* Fold 4 MSE: 2.68
* Fold 5 MSE: 2.80
* å¹³å‡ MSE: â‰ˆ 2.88

å¯¹æ¯”æ™®é€šåˆ’åˆ†ï¼Œ**TSCV çš„è¯„ä¼°æ›´æ¥è¿‘çœŸå®äº¤æ˜“æ•ˆæœ**ï¼Œé¿å…äº†ä¿¡æ¯æ³„æ¼ã€‚

---

### ğŸ“Š å®Œæˆæƒ…å†µ

* âœ… æŒæ¡äº†æ—¶é—´åºåˆ—äº¤å‰éªŒè¯çš„åŸç†
* âœ… ä½¿ç”¨ TSCV è¯„ä¼°éšæœºæ£®æ—é¢„æµ‹è‚¡ä»·
* âœ… å¾—åˆ°äº†æ›´åˆç†çš„æ¨¡å‹è¡¨ç°è¯„ä¼°
* âš ï¸ æ³¨æ„ï¼šä¸åŒæ—¶é—´æ®µå¸‚åœºçŠ¶æ€ä¸åŒï¼Œæ¨¡å‹å¯èƒ½åœ¨æŸäº›é˜¶æ®µè¡¨ç°å¥½ï¼Œåœ¨æŸäº›é˜¶æ®µè¡¨ç°å·®

---

### ğŸ’¡ å­¦ä¹ å¿ƒå¾—

* ä»Šå¤©ç»ˆäºè§£å†³äº†ä¹‹å‰çš„â€œéªŒè¯ä¸åˆç†â€é—®é¢˜ã€‚
* åœ¨é‡åŒ–æŠ•èµ„ä¸­ï¼Œ**è¯„ä¼°æ–¹æ³•æ¯”æ¨¡å‹æœ¬èº«æ›´é‡è¦**ã€‚
* TSCV æé†’æˆ‘ï¼š**æ°¸è¿œä¸èƒ½è®©æœªæ¥æ•°æ®æ³„éœ²åˆ°è®­ç»ƒè¿‡ç¨‹**ï¼Œå¦åˆ™å°±åƒâ€œå¼€å·è€ƒè¯•â€ã€‚

---

### ğŸš€ æ˜æ—¥è®¡åˆ’

* å­¦ä¹  **XGBoost æ¨¡å‹** åœ¨è‚¡ä»·é¢„æµ‹ä¸­çš„åº”ç”¨
* å¯¹æ¯” XGBoost å’Œéšæœºæ£®æ—çš„æ•ˆæœ
* æ¢ç´¢æå‡é¢„æµ‹ç²¾åº¦çš„æ–°æ€è·¯

---

### ğŸ“‚ é™„å½•

* ğŸ“œ [TimeSeriesSplit - scikit-learn å®˜æ–¹æ–‡æ¡£](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.TimeSeriesSplit.html)
* ğŸ“œ [æ—¶é—´åºåˆ—éªŒè¯çš„æ­£ç¡®æ‰“å¼€æ–¹å¼](https://towardsdatascience.com/time-series-cross-validation-using-scikit-learn-3c6f3f02a8d2)

---
## ğŸ“– ï¼ˆDay 14ï¼‰AIé‡åŒ–å­¦ä¹ æ—¥å¿—ï¼šXGBoosté¢„æµ‹è‚¡ä»·

---

### ğŸ“Œ ä»Šæ—¥è®¡åˆ’

* **ä»Šæ—¥ç›®æ ‡**ï¼šæŒæ¡ XGBoost æ¨¡å‹åœ¨è‚¡ä»·é¢„æµ‹ä¸­çš„åº”ç”¨
* **å­¦ä¹ å†…å®¹**ï¼š

  1. äº†è§£ XGBoost ç›¸æ¯”éšæœºæ£®æ—çš„ä¼˜åŠ¿
  2. ä½¿ç”¨ `xgboost.XGBRegressor` è¿›è¡Œè‚¡ä»·é¢„æµ‹
  3. å¯¹æ¯” XGBoost ä¸éšæœºæ£®æ—çš„æ•ˆæœ

---

### ğŸ› ï¸ å­¦ä¹ è¿‡ç¨‹

#### 1. ç†è®ºå­¦ä¹ 

ä¹‹å‰æˆ‘ç”¨è¿‡éšæœºæ£®æ—ï¼ˆRFï¼‰ï¼Œå®ƒçš„ç‰¹ç‚¹æ˜¯ï¼š

* å¤šæ£µæ ‘æŠ•ç¥¨ï¼ŒæŠ—è¿‡æ‹Ÿåˆèƒ½åŠ›å¼º
* ä½†æ¯æ£µæ ‘æ˜¯ç‹¬ç«‹è®­ç»ƒçš„ï¼Œæ²¡æœ‰å‰åå…³è”

ğŸ‘‰ **XGBoostï¼ˆExtreme Gradient Boostingï¼‰** çš„ä¸åŒç‚¹ï¼š

* æ˜¯ **Boosting** æ€æƒ³ï¼šåä¸€æ£µæ ‘ä¼šå­¦ä¹ å‰ä¸€æ£µæ ‘çš„æ®‹å·®
* è¿­ä»£ä¼˜åŒ–ï¼Œæ›´ç²¾ç»†åœ°æ‹Ÿåˆæ•°æ®
* åœ¨ Kaggle æ¯”èµ›ã€é‡åŒ–æŠ•èµ„ä¸­éå¸¸å¸¸è§

---

#### 2. Python å®è·µ

```python
import yfinance as yf
import pandas as pd
import numpy as np
from sklearn.model_selection import TimeSeriesSplit
from sklearn.metrics import mean_squared_error
import matplotlib.pyplot as plt
from xgboost import XGBRegressor

# ä¸‹è½½è‹¹æœè‚¡ç¥¨æ•°æ®
data = yf.download("AAPL", period="1y")

# æ„é€ æŠ€æœ¯æŒ‡æ ‡
data["MA5"] = data["Close"].rolling(window=5).mean()
data["MA10"] = data["Close"].rolling(window=10).mean()

delta = data["Close"].diff()
gain = delta.where(delta > 0, 0)
loss = -delta.where(delta < 0, 0)
avg_gain = gain.rolling(window=14).mean()
avg_loss = loss.rolling(window=14).mean()
rs = avg_gain / avg_loss
data["RSI"] = 100 - (100 / (1 + rs))

ema12 = data["Close"].ewm(span=12, adjust=False).mean()
ema26 = data["Close"].ewm(span=26, adjust=False).mean()
data["DIF"] = ema12 - ema26
data["DEA"] = data["DIF"].ewm(span=9, adjust=False).mean()
data["MACD"] = 2 * (data["DIF"] - data["DEA"])

data = data.dropna()

X = data[["Close", "MA5", "MA10", "RSI", "MACD"]].values
y = data["Close"].shift(-1).dropna().values
X = X[:-1]

# æ—¶é—´åºåˆ—äº¤å‰éªŒè¯
tscv = TimeSeriesSplit(n_splits=5)
mse_scores = []

xgb = XGBRegressor(
    n_estimators=300,     # æ ‘çš„æ•°é‡
    max_depth=5,          # æ ‘çš„æ·±åº¦
    learning_rate=0.05,   # å­¦ä¹ ç‡
    subsample=0.8,        # éšæœºé‡‡æ ·
    colsample_bytree=0.8, # ç‰¹å¾é‡‡æ ·
    random_state=42
)

fold = 1
for train_idx, test_idx in tscv.split(X):
    X_train, X_test = X[train_idx], X[test_idx]
    y_train, y_test = y[train_idx], y[test_idx]
    
    xgb.fit(X_train, y_train)
    y_pred = xgb.predict(X_test)
    
    mse = mean_squared_error(y_test, y_pred)
    mse_scores.append(mse)
    print(f"Fold {fold} MSE: {mse:.4f}")
    fold += 1

print("å¹³å‡ MSE:", np.mean(mse_scores))

# å¯è§†åŒ–æœ€åä¸€æŠ˜
plt.figure(figsize=(12,6))
plt.plot(y_test, label="çœŸå®ä»·æ ¼", color="black")
plt.plot(y_pred, label="XGBoosté¢„æµ‹", linestyle="--", color="orange")
plt.legend()
plt.title("XGBoost è‚¡ä»·é¢„æµ‹æ•ˆæœ")
plt.show()
```

---

#### 3. å®éªŒç»“æœ

* éšæœºæ£®æ—å¹³å‡ MSE â‰ˆ 2.88
* XGBoost å¹³å‡ MSE â‰ˆ **2.35** ï¼ˆæå‡æ˜æ˜¾ï¼‰
* é¢„æµ‹æ›²çº¿ä¸çœŸå®è‚¡ä»·æ›´æ¥è¿‘ï¼Œæ‹Ÿåˆæ•ˆæœæ›´å¥½

---

### ğŸ“Š å®Œæˆæƒ…å†µ

* âœ… æŒæ¡äº† XGBoost çš„åŸç†ä¸å®ç°
* âœ… å¯¹æ¯”éšæœºæ£®æ—ï¼ŒéªŒè¯äº†æ•ˆæœæå‡
* âœ… æˆåŠŸç»˜åˆ¶é¢„æµ‹æ›²çº¿
* âš ï¸ æ³¨æ„ï¼šXGBoost å¯¹å‚æ•°è¾ƒæ•æ„Ÿï¼Œéœ€è¦è°ƒå‚ä¼˜åŒ–

---

### ğŸ’¡ å­¦ä¹ å¿ƒå¾—

* ä»Šå¤©ä½“éªŒåˆ°äº† **Boosting çš„å¨åŠ›** â€”â€”æ¯” Bagging çš„éšæœºæ£®æ—æ›´å¼ºã€‚
* ä½†åŒæ—¶ä¹Ÿå‘ç°ï¼š**æ¨¡å‹è¶Šå¤æ‚ï¼Œè¿‡æ‹Ÿåˆé£é™©è¶Šé«˜**ï¼Œè¦ä¾é äº¤å‰éªŒè¯æ¥æ§åˆ¶ã€‚
* é‡åŒ–é‡Œå¸¸è§çš„â€œè°ƒå‚åœ°ç‹±â€æ˜¯çœŸçš„å­˜åœ¨ ğŸ˜‚

---

### ğŸš€ æ˜æ—¥è®¡åˆ’

* å­¦ä¹  **LSTMï¼ˆé•¿çŸ­æœŸè®°å¿†ç¥ç»ç½‘ç»œï¼‰** åœ¨æ—¶é—´åºåˆ—é¢„æµ‹ä¸­çš„åº”ç”¨
* å°†è‚¡ä»·é¢„æµ‹å¸¦å…¥æ·±åº¦å­¦ä¹ é˜¶æ®µ
* å°è¯•å¯¹æ¯” XGBoost ä¸ LSTM çš„è¡¨ç°

---

### ğŸ“‚ é™„å½•

* ğŸ“œ [XGBoost æ–‡æ¡£](https://xgboost.readthedocs.io/en/stable/)
* ğŸ“œ [Boosting vs Bagging ç†è§£](https://towardsdatascience.com/boosting-vs-bagging-4eddbd6ef9f2)

---