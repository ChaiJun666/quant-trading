---

## ğŸ“– ï¼ˆDay 15ï¼‰AIé‡åŒ–å­¦ä¹ æ—¥å¿—ï¼šLSTMé¢„æµ‹è‚¡ä»·

---

### ğŸ“Œ ä»Šæ—¥è®¡åˆ’

* **ä»Šæ—¥ç›®æ ‡**ï¼šåˆæ­¥æŒæ¡ LSTMï¼ˆé•¿çŸ­æœŸè®°å¿†ç½‘ç»œï¼‰åœ¨æ—¶é—´åºåˆ—ä¸­çš„åº”ç”¨
* **å­¦ä¹ å†…å®¹**ï¼š

  1. LSTM çš„åŸç†ä¸ä¼˜åŠ¿
  2. ç”¨ Keras/TensorFlow æ­å»º LSTM ç½‘ç»œ
  3. ä½¿ç”¨å†å²è‚¡ä»·é¢„æµ‹æœªæ¥è¶‹åŠ¿

---

### ğŸ› ï¸ å­¦ä¹ è¿‡ç¨‹

#### 1. ç†è®ºå­¦ä¹ 

* **ä¼ ç»Ÿæœºå™¨å­¦ä¹ æ¨¡å‹**ï¼ˆRFã€XGBoostï¼‰ä¸èƒ½å¾ˆå¥½æ•æ‰ **æ—¶é—´åºåˆ—ä¾èµ–æ€§**ã€‚
* **LSTMï¼ˆLong Short-Term Memoryï¼‰** æ˜¯ RNN çš„æ”¹è¿›ï¼š

  * é€šè¿‡â€œé—¨æ§æœºåˆ¶â€ï¼ˆè¾“å…¥é—¨ã€é—å¿˜é—¨ã€è¾“å‡ºé—¨ï¼‰é¿å…æ¢¯åº¦æ¶ˆå¤±
  * æ“…é•¿å¤„ç† **åºåˆ—æ•°æ®**ï¼ˆè‚¡ä»·ã€æ°”æ¸©ã€æ–‡æœ¬ç­‰ï¼‰
  * èƒ½è®°ä½é•¿æ—¶é—´çš„è¶‹åŠ¿ä¿¡æ¯

åœ¨é‡‘èæ—¶é—´åºåˆ—ä¸­ï¼ŒLSTM å¸¸è¢«ç”¨æ¥é¢„æµ‹ä¸‹ä¸€æ­¥è‚¡ä»·èµ°åŠ¿ã€‚

---

#### 2. Python å®è·µ

```python
import yfinance as yf
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.preprocessing import MinMaxScaler
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout

# ä¸‹è½½æ•°æ®
data = yf.download("AAPL", period="2y")
close_prices = data["Close"].values.reshape(-1, 1)

# æ•°æ®å½’ä¸€åŒ–
scaler = MinMaxScaler(feature_range=(0,1))
scaled_data = scaler.fit_transform(close_prices)

# æ„é€ è®­ç»ƒæ•°æ®ï¼ˆè¿‡å» 60 å¤©é¢„æµ‹ä¸‹ä¸€å¤©ï¼‰
X, y = [], []
time_step = 60
for i in range(time_step, len(scaled_data)):
    X.append(scaled_data[i-time_step:i, 0])
    y.append(scaled_data[i, 0])
X, y = np.array(X), np.array(y)

# è°ƒæ•´è¾“å…¥ç»´åº¦ [samples, time_steps, features]
X = np.reshape(X, (X.shape[0], X.shape[1], 1))

# åˆ’åˆ†è®­ç»ƒ/æµ‹è¯•é›†
split = int(0.8 * len(X))
X_train, X_test = X[:split], X[split:]
y_train, y_test = y[:split], y[split:]

# æ„å»º LSTM æ¨¡å‹
model = Sequential([
    LSTM(50, return_sequences=True, input_shape=(X.shape[1], 1)),
    Dropout(0.2),
    LSTM(50, return_sequences=False),
    Dropout(0.2),
    Dense(25),
    Dense(1)
])

model.compile(optimizer="adam", loss="mean_squared_error")

# è®­ç»ƒæ¨¡å‹
history = model.fit(X_train, y_train, epochs=20, batch_size=32, validation_data=(X_test, y_test), verbose=1)

# é¢„æµ‹
predictions = model.predict(X_test)
predictions = scaler.inverse_transform(predictions.reshape(-1,1))

real_prices = scaler.inverse_transform(y_test.reshape(-1,1))

# å¯è§†åŒ–
plt.figure(figsize=(12,6))
plt.plot(real_prices, label="çœŸå®ä»·æ ¼", color="black")
plt.plot(predictions, label="LSTMé¢„æµ‹", color="blue", linestyle="--")
plt.legend()
plt.title("LSTM è‚¡ä»·é¢„æµ‹æ•ˆæœ")
plt.show()
```

---

#### 3. å®éªŒç»“æœ

* LSTM èƒ½æ›´å¥½åœ°æ•æ‰è‚¡ä»·è¶‹åŠ¿ï¼Œé¢„æµ‹æ›²çº¿æ›´åŠ â€œå¹³æ»‘â€
* ä½†å¯¹çŸ­æœŸæ³¢åŠ¨æ•æ„Ÿåº¦ä¸è¶³ï¼ˆé¢„æµ‹æ›´åƒæ˜¯è¶‹åŠ¿çº¿ï¼Œè€Œä¸æ˜¯ç‚¹å¯¹ç‚¹æ‹Ÿåˆï¼‰
* æŸå¤±å‡½æ•°æ”¶æ•›è¾ƒå¿«ï¼ŒéªŒè¯é›†ä¸è®­ç»ƒé›†è¡¨ç°æ¥è¿‘ï¼Œè¯´æ˜æ²¡æœ‰æ˜æ˜¾è¿‡æ‹Ÿåˆ

---

### ğŸ“Š å®Œæˆæƒ…å†µ

* âœ… æŒæ¡äº† LSTM çš„åŸºæœ¬åŸç†
* âœ… æˆåŠŸæ­å»º LSTM å¹¶è¿›è¡Œè‚¡ä»·é¢„æµ‹
* âœ… ç»˜åˆ¶é¢„æµ‹æ›²çº¿ï¼Œæ•ˆæœè¾ƒä¸ºç†æƒ³
* âš ï¸ æ³¨æ„ï¼šLSTM è®­ç»ƒè€—æ—¶è¾ƒé•¿ï¼Œå‚æ•°ï¼ˆæ—¶é—´çª—å£ã€å±‚æ•°ã€ç¥ç»å…ƒæ•°ï¼‰å½±å“å¾ˆå¤§

---

### ğŸ’¡ å­¦ä¹ å¿ƒå¾—

* LSTM è®©æˆ‘ç¬¬ä¸€æ¬¡æ„Ÿå—åˆ° **æ·±åº¦å­¦ä¹ é¢„æµ‹æ—¶é—´åºåˆ—çš„é­…åŠ›**ã€‚
* å’Œ XGBoost ç›¸æ¯”ï¼Œå®ƒèƒ½æ•æ‰åˆ°â€œè¶‹åŠ¿â€ï¼Œä½†çŸ­æœŸæ³¢åŠ¨é¢„æµ‹è¿˜ä¸å¤Ÿç²¾å‡†ã€‚
* åœ¨é‡åŒ–å®ç›˜ä¸­ï¼Œå¯èƒ½éœ€è¦ç»“åˆ **æœºå™¨å­¦ä¹ ï¼ˆçŸ­æœŸï¼‰+ LSTMï¼ˆè¶‹åŠ¿ï¼‰**ï¼Œå½¢æˆæ›´ç¨³å¥çš„é¢„æµ‹ç³»ç»Ÿã€‚

---

### ğŸš€ æ˜æ—¥è®¡åˆ’

* å°è¯• **ä¼˜åŒ– LSTM**ï¼ˆå¢åŠ å±‚æ•°ã€è°ƒæ•´æ—¶é—´çª—å£ï¼‰
* å­¦ä¹  **GRUï¼ˆé—¨æ§å¾ªç¯å•å…ƒï¼‰**ï¼Œå’Œ LSTM å¯¹æ¯”
* å¼€å§‹è€ƒè™‘å¦‚ä½•æŠŠé¢„æµ‹ç»“æœè½¬åŒ–ä¸º **äº¤æ˜“ä¿¡å·**

---

### ğŸ“‚ é™„å½•

* ğŸ“œ [LSTM åŸç†è¯¦è§£](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)
* ğŸ“œ [Keras LSTM å®˜æ–¹æ–‡æ¡£](https://keras.io/api/layers/recurrent_layers/lstm/)

---

## ğŸ“– ï¼ˆDay 16ï¼‰AIé‡åŒ–å­¦ä¹ æ—¥å¿—ï¼šGRUä¸LSTMå¯¹æ¯”

---

### ğŸ“Œ ä»Šæ—¥è®¡åˆ’

* **ä»Šæ—¥ç›®æ ‡**ï¼šå­¦ä¹  GRUï¼ˆé—¨æ§å¾ªç¯å•å…ƒï¼‰çš„åŸç†ï¼Œå¹¶ä¸ LSTM è¿›è¡Œå¯¹æ¯”
* **å­¦ä¹ å†…å®¹**ï¼š

  1. GRU çš„ç»“æ„ä¸ä¼˜åŠ¿
  2. ç”¨ Keras æ­å»º GRU ç½‘ç»œè¿›è¡Œè‚¡ä»·é¢„æµ‹
  3. å¯¹æ¯” GRU ä¸ LSTM åœ¨è‚¡ä»·é¢„æµ‹ä¸­çš„è¡¨ç°

---

### ğŸ› ï¸ å­¦ä¹ è¿‡ç¨‹

#### 1. ç†è®ºå­¦ä¹ 

åœ¨æ·±åº¦å­¦ä¹ é¢„æµ‹è‚¡ä»·æ—¶ï¼Œ**LSTM** æ˜¯å¸¸ç”¨é€‰æ‹©ã€‚ä½† LSTM ç»“æ„å¤æ‚ï¼ŒåŒ…å«ï¼š

* è¾“å…¥é—¨
* é—å¿˜é—¨
* è¾“å‡ºé—¨

ğŸ‘‰ **GRUï¼ˆGated Recurrent Unitï¼‰** æ˜¯ LSTM çš„ç®€åŒ–ç‰ˆæœ¬ï¼š

* åªæœ‰ **æ›´æ–°é—¨ï¼ˆUpdate Gateï¼‰** å’Œ **é‡ç½®é—¨ï¼ˆReset Gateï¼‰**
* è®¡ç®—é‡æ›´å°ï¼Œè®­ç»ƒé€Ÿåº¦æ›´å¿«
* åœ¨å¾ˆå¤šä»»åŠ¡ä¸­ï¼Œæ€§èƒ½ä¸ LSTM ç›¸å½“

å¯ä»¥ç†è§£ä¸ºï¼š**GRU æ˜¯è½»é‡ç‰ˆ LSTM**ã€‚

---

#### 2. Python å®è·µ

```python
import yfinance as yf
import numpy as np
import matplotlib.pyplot as plt
from sklearn.preprocessing import MinMaxScaler
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import GRU, LSTM, Dense, Dropout

# ä¸‹è½½è‚¡ç¥¨æ•°æ®
data = yf.download("AAPL", period="2y")
close_prices = data["Close"].values.reshape(-1,1)

# å½’ä¸€åŒ–
scaler = MinMaxScaler(feature_range=(0,1))
scaled_data = scaler.fit_transform(close_prices)

# æ„é€ åºåˆ—æ•°æ®
time_step = 60
X, y = [], []
for i in range(time_step, len(scaled_data)):
    X.append(scaled_data[i-time_step:i, 0])
    y.append(scaled_data[i, 0])
X, y = np.array(X), np.array(y)
X = np.reshape(X, (X.shape[0], X.shape[1], 1))

# åˆ’åˆ†è®­ç»ƒ/æµ‹è¯•é›†
split = int(0.8 * len(X))
X_train, X_test = X[:split], X[split:]
y_train, y_test = y[:split], y[split:]

# æ„å»º GRU æ¨¡å‹
gru_model = Sequential([
    GRU(50, return_sequences=True, input_shape=(X.shape[1], 1)),
    Dropout(0.2),
    GRU(50, return_sequences=False),
    Dropout(0.2),
    Dense(25),
    Dense(1)
])
gru_model.compile(optimizer="adam", loss="mean_squared_error")

# æ„å»º LSTM æ¨¡å‹ï¼ˆç”¨äºå¯¹æ¯”ï¼‰
lstm_model = Sequential([
    LSTM(50, return_sequences=True, input_shape=(X.shape[1], 1)),
    Dropout(0.2),
    LSTM(50, return_sequences=False),
    Dropout(0.2),
    Dense(25),
    Dense(1)
])
lstm_model.compile(optimizer="adam", loss="mean_squared_error")

# åˆ†åˆ«è®­ç»ƒ
gru_model.fit(X_train, y_train, epochs=10, batch_size=32, verbose=1)
lstm_model.fit(X_train, y_train, epochs=10, batch_size=32, verbose=1)

# é¢„æµ‹
gru_pred = scaler.inverse_transform(gru_model.predict(X_test))
lstm_pred = scaler.inverse_transform(lstm_model.predict(X_test))
real_prices = scaler.inverse_transform(y_test.reshape(-1,1))

# å¯è§†åŒ–å¯¹æ¯”
plt.figure(figsize=(12,6))
plt.plot(real_prices, label="çœŸå®ä»·æ ¼", color="black")
plt.plot(lstm_pred, label="LSTMé¢„æµ‹", linestyle="--", color="blue")
plt.plot(gru_pred, label="GRUé¢„æµ‹", linestyle="--", color="orange")
plt.legend()
plt.title("GRU vs LSTM è‚¡ä»·é¢„æµ‹")
plt.show()
```

---

#### 3. å®éªŒç»“æœ

* **LSTM**ï¼šæ‹Ÿåˆæ•ˆæœæ›´ç¨³å®šï¼Œèƒ½æ•æ‰åˆ°è¾ƒé•¿æœŸè¶‹åŠ¿
* **GRU**ï¼šè®­ç»ƒé€Ÿåº¦å¿«ï¼Œç»“æœå’Œ LSTM ç›¸è¿‘ï¼Œæœ‰æ—¶ç”šè‡³æ›´å¥½
* åœ¨è‹¹æœè‚¡ä»·æ•°æ®ä¸­ï¼ŒGRU æ›²çº¿æ¯” LSTM æ›´â€œè´´åˆâ€ï¼Œä½† LSTM æ›´å¹³æ»‘

---

### ğŸ“Š å®Œæˆæƒ…å†µ

* âœ… å­¦ä¹ äº† GRU çš„åŸç†
* âœ… æˆåŠŸå®ç°äº† GRU å’Œ LSTM çš„å¯¹æ¯”å®éªŒ
* âœ… ç»˜åˆ¶äº†é¢„æµ‹æ›²çº¿å¹¶è¿›è¡Œäº†åˆ†æ
* âš ï¸ å‘ç°ï¼šä¸åŒæ•°æ®é›†ä¸‹ï¼ŒGRU å’Œ LSTM çš„è¡¨ç°å¯èƒ½äº’æœ‰èƒœè´Ÿ

---

### ğŸ’¡ å­¦ä¹ å¿ƒå¾—

* **LSTM â‰ˆ ç²¾ç»†ï¼ŒGRU â‰ˆ é«˜æ•ˆ**
* åœ¨é‡‘èåœºæ™¯ä¸­ï¼Œå¦‚æœæ•°æ®é‡å¾ˆå¤§ï¼ŒGRU æ›´å®ç”¨ï¼›å¦‚æœè¦æ•æ‰æ›´å¤æ‚çš„é•¿æœŸä¾èµ–ï¼ŒLSTM æ›´åˆé€‚
* ä»Šå¤©çš„å¯¹æ¯”è®©æˆ‘æ›´ç†è§£äº†ä¸ºä»€ä¹ˆå¾ˆå¤šè®ºæ–‡ä¼šç”¨ GRU æ›¿ä»£ LSTMï¼š**å·®ä¸å¤šçš„æ•ˆæœï¼Œçœä¸‹ä¸å°‘ç®—åŠ›**

---

### ğŸš€ æ˜æ—¥è®¡åˆ’

* å­¦ä¹  **æ··åˆæ¨¡å‹**ï¼šæŠŠ LSTM/GRU ä¸ **å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰** ç»“åˆ
* å°è¯•æ„å»º **CNN-LSTM** ç½‘ç»œï¼Œæé«˜é¢„æµ‹èƒ½åŠ›
* æ¢ç´¢å¦‚ä½•æŠŠé¢„æµ‹ç»“æœè½¬åŒ–ä¸º **ä¹°å–ä¿¡å·**

---

### ğŸ“‚ é™„å½•

* ğŸ“œ [Understanding GRU Networks](https://towardsdatascience.com/understanding-gru-networks-2ef37df6c9be)
* ğŸ“œ [GRU vs LSTM: A Comparison](https://arxiv.org/abs/1412.3555)


---

## ğŸ“– ï¼ˆDay 17ï¼‰AIé‡åŒ–å­¦ä¹ æ—¥å¿—ï¼šCNN-LSTMæ··åˆæ¨¡å‹

---

### ğŸ“Œ ä»Šæ—¥è®¡åˆ’

* **ä»Šæ—¥ç›®æ ‡**ï¼šå­¦ä¹  CNN ä¸ LSTM ç»“åˆçš„æ··åˆæ¨¡å‹
* **å­¦ä¹ å†…å®¹**ï¼š

  1. CNN æå–ç‰¹å¾ + LSTM å¤„ç†æ—¶é—´ä¾èµ–çš„åŸç†
  2. ç”¨ Keras æ­å»º CNN-LSTM æ¨¡å‹
  3. å¯¹æ¯” LSTMã€GRU ä¸ CNN-LSTM çš„è¡¨ç°

---

### ğŸ› ï¸ å­¦ä¹ è¿‡ç¨‹

#### 1. ç†è®ºå­¦ä¹ 

* **CNNï¼ˆå·ç§¯ç¥ç»ç½‘ç»œï¼‰**ï¼šæœ¬æ¥å¸¸ç”¨äºå›¾åƒè¯†åˆ«ï¼Œä½†åœ¨æ—¶é—´åºåˆ—ä¸­ä¹Ÿå¯ä»¥æå–å±€éƒ¨æ¨¡å¼ï¼ˆæ¯”å¦‚çŸ­æœŸæ³¢åŠ¨ç‰¹å¾ï¼‰ã€‚
* **LSTM**ï¼šé€‚åˆæ•æ‰é•¿æœŸä¾èµ–å…³ç³»ã€‚
* **CNN-LSTM æ··åˆæ¨¡å‹**æ€è·¯ï¼š

  * CNN æå–è‚¡ä»·çš„å±€éƒ¨ç‰¹å¾
  * LSTM å†å»ºæ¨¡è¿™äº›ç‰¹å¾çš„æ—¶åºä¾èµ–
  * æœ€ç»ˆå¾—åˆ°æ›´é²æ£’çš„é¢„æµ‹

ğŸ‘‰ é€šä¿—ç†è§£ï¼š**CNN-LSTM = ç‰¹å¾æå– + æ—¶åºå»ºæ¨¡**ã€‚

---

#### 2. Python å®è·µ

```python
import yfinance as yf
import numpy as np
import matplotlib.pyplot as plt
from sklearn.preprocessing import MinMaxScaler
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv1D, MaxPooling1D, LSTM, Dense, Dropout, Flatten, TimeDistributed

# ä¸‹è½½æ•°æ®
data = yf.download("AAPL", period="2y")
close_prices = data["Close"].values.reshape(-1,1)

# å½’ä¸€åŒ–
scaler = MinMaxScaler(feature_range=(0,1))
scaled_data = scaler.fit_transform(close_prices)

# æ„é€ è®­ç»ƒæ•°æ®
time_step = 60
X, y = [], []
for i in range(time_step, len(scaled_data)):
    X.append(scaled_data[i-time_step:i, 0])
    y.append(scaled_data[i, 0])
X, y = np.array(X), np.array(y)

# è¾“å…¥ç»´åº¦è°ƒæ•´ä¸º CNN-LSTM å¯æ¥å—æ ¼å¼
X = np.reshape(X, (X.shape[0], X.shape[1], 1))

# åˆ’åˆ†è®­ç»ƒ/æµ‹è¯•é›†
split = int(0.8 * len(X))
X_train, X_test = X[:split], X[split:]
y_train, y_test = y[:split], y[split:]

# æ„å»º CNN-LSTM æ¨¡å‹
model = Sequential([
    Conv1D(filters=64, kernel_size=3, activation="relu", input_shape=(X.shape[1], 1)),
    MaxPooling1D(pool_size=2),
    Dropout(0.2),
    LSTM(50, return_sequences=False),
    Dropout(0.2),
    Dense(25, activation="relu"),
    Dense(1)
])

model.compile(optimizer="adam", loss="mean_squared_error")

# è®­ç»ƒ
history = model.fit(X_train, y_train, epochs=15, batch_size=32, validation_data=(X_test, y_test), verbose=1)

# é¢„æµ‹
predictions = model.predict(X_test)
predictions = scaler.inverse_transform(predictions.reshape(-1,1))
real_prices = scaler.inverse_transform(y_test.reshape(-1,1))

# å¯è§†åŒ–
plt.figure(figsize=(12,6))
plt.plot(real_prices, label="çœŸå®ä»·æ ¼", color="black")
plt.plot(predictions, label="CNN-LSTMé¢„æµ‹", linestyle="--", color="red")
plt.legend()
plt.title("CNN-LSTM è‚¡ä»·é¢„æµ‹æ•ˆæœ")
plt.show()
```

---

#### 3. å®éªŒç»“æœ

* CNN-LSTM é¢„æµ‹æ›²çº¿æ¯”å•çº¯çš„ LSTM æ›´â€œè´´åˆâ€çŸ­æœŸæ³¢åŠ¨
* éªŒè¯é›†æŸå¤±ä¸‹é™é€Ÿåº¦æ¯” LSTM ç•¥å¿«
* åœ¨æ•æ‰çŸ­æœŸè¶‹åŠ¿æ–¹é¢ï¼ŒCNN-LSTM è¡¨ç°ä¼˜äº LSTM å’Œ GRU

---

### ğŸ“Š å®Œæˆæƒ…å†µ

* âœ… æŒæ¡äº† CNN-LSTM çš„åŸç†
* âœ… æˆåŠŸå®ç°å¹¶è®­ç»ƒæ¨¡å‹
* âœ… ç»˜åˆ¶é¢„æµ‹æ›²çº¿å¹¶è§‚å¯Ÿæ•ˆæœ
* âš ï¸ éœ€è¦æ³¨æ„ï¼šå·ç§¯æ ¸å¤§å°ã€LSTM å±‚æ•°ç­‰è¶…å‚æ•°ä¼šæ˜¾è‘—å½±å“ç»“æœ

---

### ğŸ’¡ å­¦ä¹ å¿ƒå¾—

* ä»Šå¤©çš„ CNN-LSTM å®éªŒè®©æˆ‘ç¬¬ä¸€æ¬¡çœ‹åˆ°â€œæ··åˆæ¨¡å‹â€çš„å¼ºå¤§ï¼š

  * CNN èƒ½æŠ“åˆ°å±€éƒ¨æ¨¡å¼
  * LSTM èƒ½æŠŠå±€éƒ¨æ¨¡å¼ä¸²èµ·æ¥
* åœ¨è‚¡ä»·é¢„æµ‹ä¸­ï¼Œè¿™æ ·çš„ç»„åˆèƒ½åŒæ—¶å…¼é¡¾ **çŸ­æœŸæ³¢åŠ¨** ä¸ **é•¿æœŸè¶‹åŠ¿**ã€‚
* æœªæ¥å¯ä»¥å°è¯• **CNN-GRU** æˆ–æ›´å¤æ‚çš„ **Attention + LSTM** ç»“æ„ã€‚

---

### ğŸš€ æ˜æ—¥è®¡åˆ’

* å­¦ä¹  **Attention æœºåˆ¶** çš„åŸºæœ¬åŸç†
* å°è¯•åœ¨ LSTM ä¸ŠåŠ å…¥ Attention
* æ¢ç´¢ **LSTM-Attention æ¨¡å‹** åœ¨è‚¡ä»·é¢„æµ‹ä¸­çš„è¡¨ç°

---

### ğŸ“‚ é™„å½•

* ğŸ“œ [CNN for Time Series](https://towardsdatascience.com/convolutional-neural-networks-for-time-series-classification-4d447fcbf3e)
* ğŸ“œ [CNN-LSTM Keras Example](https://machinelearningmastery.com/cnn-long-short-term-memory-networks/)

---

## ğŸ“– ï¼ˆDay 18ï¼‰AIé‡åŒ–å­¦ä¹ æ—¥å¿—ï¼šLSTM + Attention æ¨¡å‹

---

### ğŸ“Œ ä»Šæ—¥è®¡åˆ’

* **ä»Šæ—¥ç›®æ ‡**ï¼šå­¦ä¹  Attention æœºåˆ¶ï¼Œå¹¶å°†å…¶åº”ç”¨åˆ° LSTM è‚¡ä»·é¢„æµ‹ä¸­
* **å­¦ä¹ å†…å®¹**ï¼š

  1. Attention çš„åŸºæœ¬åŸç†
  2. åœ¨ LSTM æ¨¡å‹ä¸­å¼•å…¥ Attention å±‚
  3. å¯¹æ¯” LSTM ä¸ LSTM+Attention çš„é¢„æµ‹æ•ˆæœ

---

### ğŸ› ï¸ å­¦ä¹ è¿‡ç¨‹

#### 1. ç†è®ºå­¦ä¹ 

* **LSTM çš„å±€é™**ï¼šè™½ç„¶èƒ½æ•æ‰æ—¶é—´ä¾èµ–ï¼Œä½†å¯¹ **é•¿åºåˆ—ä¸­ä¸åŒæ—¶é—´ç‚¹çš„é‡è¦æ€§** æ— æ³•åŒºåˆ†ã€‚
* **Attention æœºåˆ¶**ï¼šè®©æ¨¡å‹åœ¨é¢„æµ‹æ—¶ï¼Œå¯¹ä¸åŒæ—¶é—´ç‚¹çš„â€œæƒé‡â€è¿›è¡Œåˆ†é…ã€‚
* é€šä¿—ç†è§£ï¼š

  * LSTM æ˜¯â€œçœ‹å®Œæ•´æ®µå†å²ï¼Œå†æ€»ç»“â€
  * Attention æ˜¯â€œæŒ‘å‡ºå…³é”®çš„å†å²ç‰‡æ®µï¼Œæ›´ç²¾å‡†åœ°æ€»ç»“â€

ğŸ‘‰ åœ¨è‚¡ä»·é¢„æµ‹ä¸­ï¼ŒæŸäº›å¤©çš„æ³¢åŠ¨å¯èƒ½æ¯”å…¶ä»–å¤©æ›´å…³é”®ï¼ŒAttention èƒ½å¸®åŠ©æ¨¡å‹â€œèšç„¦â€åˆ°è¿™äº›é‡è¦ç‚¹ã€‚

---

#### 2. Python å®è·µ

```python
import yfinance as yf
import numpy as np
import matplotlib.pyplot as plt
from sklearn.preprocessing import MinMaxScaler
from tensorflow.keras.models import Model
from tensorflow.keras.layers import LSTM, Dense, Dropout, Input, Attention

# ä¸‹è½½è‚¡ç¥¨æ•°æ®
data = yf.download("AAPL", period="2y")
close_prices = data["Close"].values.reshape(-1,1)

# æ•°æ®å½’ä¸€åŒ–
scaler = MinMaxScaler(feature_range=(0,1))
scaled_data = scaler.fit_transform(close_prices)

# æ„é€ è®­ç»ƒæ•°æ®
time_step = 60
X, y = [], []
for i in range(time_step, len(scaled_data)):
    X.append(scaled_data[i-time_step:i, 0])
    y.append(scaled_data[i, 0])
X, y = np.array(X), np.array(y)
X = np.reshape(X, (X.shape[0], X.shape[1], 1))

# åˆ’åˆ†è®­ç»ƒ/æµ‹è¯•é›†
split = int(0.8 * len(X))
X_train, X_test = X[:split], X[split:]
y_train, y_test = y[:split], y[split:]

# å®šä¹‰ LSTM + Attention æ¨¡å‹
inputs = Input(shape=(X.shape[1], 1))
lstm_out = LSTM(50, return_sequences=True)(inputs)
attention_out = Attention()([lstm_out, lstm_out])  # è‡ªæ³¨æ„åŠ›
lstm_out2 = LSTM(50, return_sequences=False)(attention_out)
drop = Dropout(0.2)(lstm_out2)
dense1 = Dense(25, activation="relu")(drop)
outputs = Dense(1)(dense1)

model = Model(inputs, outputs)
model.compile(optimizer="adam", loss="mean_squared_error")

# è®­ç»ƒ
history = model.fit(X_train, y_train, epochs=15, batch_size=32, validation_data=(X_test, y_test), verbose=1)

# é¢„æµ‹
predictions = model.predict(X_test)
predictions = scaler.inverse_transform(predictions.reshape(-1,1))
real_prices = scaler.inverse_transform(y_test.reshape(-1,1))

# å¯è§†åŒ–
plt.figure(figsize=(12,6))
plt.plot(real_prices, label="çœŸå®ä»·æ ¼", color="black")
plt.plot(predictions, label="LSTM+Attentioné¢„æµ‹", linestyle="--", color="purple")
plt.legend()
plt.title("LSTM+Attention è‚¡ä»·é¢„æµ‹æ•ˆæœ")
plt.show()
```

---

#### 3. å®éªŒç»“æœ

* **LSTM+Attention** çš„é¢„æµ‹æ›²çº¿æ›´æ¥è¿‘çœŸå®èµ°åŠ¿
* Attention ä½¿å¾—æ¨¡å‹åœ¨å…³é”®ç‚¹ï¼ˆå¤§æ¶¨å¤§è·Œæ—¥ï¼‰ä¸Šé¢„æµ‹æ•ˆæœæ›´å¥½
* éªŒè¯é›†æŸå¤±æ¯”çº¯ LSTM æ›´ä½ï¼Œè¯´æ˜æ³›åŒ–èƒ½åŠ›å¢å¼º

---

### ğŸ“Š å®Œæˆæƒ…å†µ

* âœ… æŒæ¡äº† Attention çš„åŸç†
* âœ… æˆåŠŸå®ç° LSTM+Attention æ¨¡å‹
* âœ… é¢„æµ‹æ•ˆæœæå‡ï¼Œå°¤å…¶åœ¨æ³¢åŠ¨è¾ƒå¤§çš„åŒºé—´
* âš ï¸ éœ€è¦æ³¨æ„ï¼šAttention å¢åŠ äº†è®¡ç®—é‡ï¼Œè®­ç»ƒæ›´æ…¢

---

### ğŸ’¡ å­¦ä¹ å¿ƒå¾—

* Attention æœºåˆ¶è®©æˆ‘æ„è¯†åˆ°ï¼Œ**ä¸æ˜¯æ‰€æœ‰å†å²æ•°æ®éƒ½ä¸€æ ·é‡è¦**ã€‚
* åœ¨è‚¡å¸‚è¿™ç§æ³¢åŠ¨å¼ºçƒˆçš„åœºæ™¯ä¸‹ï¼ŒAttention èƒ½å¸®åŠ©æ¨¡å‹èšç„¦åˆ°å…³é”®è¡Œæƒ…ã€‚
* ä»Šå¤©çš„å®éªŒè¿›ä¸€æ­¥è¯æ˜äº† **æ·±åº¦å­¦ä¹  + æ³¨æ„åŠ›æœºåˆ¶** çš„æ½œåŠ›ï¼Œåé¢ç”šè‡³å¯ä»¥å°è¯• **Transformer**ã€‚

---

### ğŸš€ æ˜æ—¥è®¡åˆ’

* å­¦ä¹  **Transformer** çš„åŸºæœ¬ç»“æ„ï¼ˆEncoder-Decoderï¼‰
* å°è¯•ç”¨ **Transformer Encoder** æ¥é¢„æµ‹è‚¡ä»·
* ä¸ LSTM+Attention è¿›è¡Œå¯¹æ¯”

---

### ğŸ“‚ é™„å½•

* ğŸ“œ [Attention is All You Need](https://arxiv.org/abs/1706.03762)
* ğŸ“œ [Keras Attention Layer](https://keras.io/api/layers/attention_layers/attention/)


---

## ğŸ“– ï¼ˆDay 19ï¼‰AIé‡åŒ–å­¦ä¹ æ—¥å¿—ï¼šTransformer è‚¡ä»·é¢„æµ‹

---

### ğŸ“Œ ä»Šæ—¥è®¡åˆ’

* **ä»Šæ—¥ç›®æ ‡**ï¼šç†è§£ Transformer çš„åŸºæœ¬ç»“æ„ï¼Œå¹¶å®ç°ä¸€ä¸ªåŸºäº **Transformer Encoder** çš„è‚¡ä»·é¢„æµ‹æ¨¡å‹
* **å­¦ä¹ å†…å®¹**ï¼š

  1. Transformer çš„ Encoder ç»“æ„
  2. è‡ªæ³¨æ„åŠ›ï¼ˆSelf-Attentionï¼‰åœ¨æ—¶é—´åºåˆ—é¢„æµ‹ä¸­çš„ä½œç”¨
  3. æ­å»º Transformer Encoder + å…¨è¿æ¥å±‚çš„é¢„æµ‹æ¨¡å‹

---

### ğŸ› ï¸ å­¦ä¹ è¿‡ç¨‹

#### 1. ç†è®ºå­¦ä¹ 

* **LSTM+Attention çš„ä¸è¶³**ï¼šLSTM åºåˆ—ä¾èµ–å¼ºï¼Œè®­ç»ƒé€Ÿåº¦è¾ƒæ…¢ã€‚
* **Transformer ä¼˜åŠ¿**ï¼šå®Œå…¨åŸºäº Attention æœºåˆ¶ï¼Œæ”¯æŒå¹¶è¡Œè®­ç»ƒï¼Œèƒ½æ›´å¥½åœ°æ•æ‰è¿œè·ç¦»ä¾èµ–ã€‚
* **æ ¸å¿ƒç»“æ„**ï¼š

  * **Self-Attention**ï¼šæ•æ‰åºåˆ—ä¸­ä¸åŒä½ç½®ä¹‹é—´çš„å…³ç³»
  * **Feed Forward Layer**ï¼šéçº¿æ€§æ˜ å°„
  * **LayerNorm + æ®‹å·®è¿æ¥**ï¼šè®©è®­ç»ƒæ›´ç¨³å®š

ğŸ‘‰ é€šä¿—ç†è§£ï¼š
LSTM åƒæ˜¯â€œé¡ºåºé˜…è¯»â€ï¼Œè€Œ Transformer æ˜¯â€œå…¨å±€æ‰«è§†â€ï¼Œä¸€çœ¼å°±èƒ½çœ‹åˆ°æ‰€æœ‰å†å²ç‚¹çš„è”ç³»ã€‚

---

#### 2. Python å®è·µ

```python
import yfinance as yf
import numpy as np
import matplotlib.pyplot as plt
from sklearn.preprocessing import MinMaxScaler
import tensorflow as tf
from tensorflow.keras import layers, Model

# ä¸‹è½½æ•°æ®
data = yf.download("AAPL", period="2y")
close_prices = data["Close"].values.reshape(-1,1)

# æ•°æ®å½’ä¸€åŒ–
scaler = MinMaxScaler(feature_range=(0,1))
scaled_data = scaler.fit_transform(close_prices)

# æ„é€ æ—¶é—´åºåˆ—æ•°æ®
time_step = 60
X, y = [], []
for i in range(time_step, len(scaled_data)):
    X.append(scaled_data[i-time_step:i, 0])
    y.append(scaled_data[i, 0])
X, y = np.array(X), np.array(y)
X = np.reshape(X, (X.shape[0], X.shape[1], 1))

# è®­ç»ƒ/æµ‹è¯•é›†
split = int(0.8 * len(X))
X_train, X_test = X[:split], X[split:]
y_train, y_test = y[:split], y[split:]

# ä½ç½®ç¼–ç ï¼ˆå¯ç®€åŒ–ï¼‰
def positional_encoding(seq_len, d_model):
    positions = np.arange(seq_len)[:, np.newaxis]
    dims = np.arange(d_model)[np.newaxis, :]
    angle_rates = 1 / np.power(10000, (2 * (dims//2)) / np.float32(d_model))
    angle_rads = positions * angle_rates
    sines = np.sin(angle_rads[:, 0::2])
    cosines = np.cos(angle_rads[:, 1::2])
    pos_encoding = np.concatenate([sines, cosines], axis=-1)
    return tf.cast(pos_encoding[np.newaxis, ...], dtype=tf.float32)

# Transformer Encoder æ¨¡å—
def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout=0):
    x = layers.MultiHeadAttention(key_dim=head_size, num_heads=num_heads)(inputs, inputs)
    x = layers.Dropout(dropout)(x)
    x = layers.LayerNormalization(epsilon=1e-6)(x + inputs)
    ff = layers.Dense(ff_dim, activation="relu")(x)
    ff = layers.Dense(inputs.shape[-1])(ff)
    x = layers.LayerNormalization(epsilon=1e-6)(x + ff)
    return x

# æ„å»ºæ¨¡å‹
inputs = layers.Input(shape=(time_step, 1))
x = transformer_encoder(inputs, head_size=32, num_heads=4, ff_dim=64, dropout=0.1)
x = layers.GlobalAveragePooling1D()(x)
x = layers.Dropout(0.2)(x)
x = layers.Dense(32, activation="relu")(x)
outputs = layers.Dense(1)(x)

model = Model(inputs, outputs)
model.compile(optimizer="adam", loss="mse")

# è®­ç»ƒ
history = model.fit(X_train, y_train, epochs=15, batch_size=32, validation_data=(X_test, y_test), verbose=1)

# é¢„æµ‹
predictions = model.predict(X_test)
predictions = scaler.inverse_transform(predictions)
real_prices = scaler.inverse_transform(y_test.reshape(-1,1))

# å¯è§†åŒ–
plt.figure(figsize=(12,6))
plt.plot(real_prices, label="çœŸå®ä»·æ ¼", color="black")
plt.plot(predictions, label="Transformeré¢„æµ‹", linestyle="--", color="orange")
plt.legend()
plt.title("Transformer è‚¡ä»·é¢„æµ‹æ•ˆæœ")
plt.show()
```

---

#### 3. å®éªŒç»“æœ

* **é¢„æµ‹èµ°åŠ¿æ›´åŠ å¹³æ»‘**ï¼Œä½†åœ¨æç«¯æ³¢åŠ¨ç‚¹ä¸Šç•¥æ˜¾æ»å
* åœ¨éªŒè¯é›†ä¸Šï¼ŒTransformer çš„æ”¶æ•›é€Ÿåº¦ **æ¯” LSTM æ›´å¿«**
* å¯¹é•¿åºåˆ—çš„ä¾èµ–æ•æ‰æ˜æ˜¾ä¼˜äº LSTM

---

### ğŸ“Š å®Œæˆæƒ…å†µ

* âœ… ç†è§£äº† Transformer Encoder çš„ç»“æ„
* âœ… æ­å»ºäº†ç¬¬ä¸€ä¸ª Transformer è‚¡ä»·é¢„æµ‹æ¨¡å‹
* âœ… ä¸ LSTM+Attention è¿›è¡Œäº†å¯¹æ¯”ï¼Œé€Ÿåº¦æ›´å¿«ï¼Œæ•ˆæœæ›´ç¨³
* âš ï¸ æ³¨æ„ï¼šéœ€è¦æ›´å¤šæ•°æ®å’Œè°ƒå‚ï¼Œé¿å…è¿‡æ‹Ÿåˆ

---

### ğŸ’¡ å­¦ä¹ å¿ƒå¾—

* Transformer è®©æˆ‘çœŸæ­£ä½“ä¼šåˆ° **Attention is All You Need** çš„é­…åŠ›ã€‚
* é€šè¿‡å…¨å±€è‡ªæ³¨æ„åŠ›æœºåˆ¶ï¼Œæ¨¡å‹èƒ½åœ¨æ›´å¤§èŒƒå›´å†…æ•æ‰è‚¡ä»·èµ°åŠ¿çš„å…³è”æ€§ã€‚
* ä»Šå¤©æ˜¯ç¬¬ä¸€æ¬¡å°è¯•ï¼Œåç»­è¿˜å¯ä»¥åŠ å…¥ **æ›´æ·±çš„ Encoder å±‚æ•°ã€æ­£åˆ™åŒ–æŠ€å·§** æ¥æå‡æ•ˆæœã€‚

---

### ğŸš€ æ˜æ—¥è®¡åˆ’

* å­¦ä¹  **å¤šå±‚ Transformer Encoder å †å **
* å°è¯• **Transformer vs LSTM+Attention çš„ç³»ç»Ÿå¯¹æ¯”å®éªŒ**
* åŠ å…¥ **æ›´å¤šç‰¹å¾ï¼ˆæˆäº¤é‡ã€æŠ€æœ¯æŒ‡æ ‡ï¼‰**ï¼Œè®©æ¨¡å‹æ›´è´´è¿‘å®æˆ˜

---

### ğŸ“‚ é™„å½•

* ğŸ“œ [Attention is All You Need](https://arxiv.org/abs/1706.03762)
* ğŸ“œ [Keras MultiHeadAttention](https://keras.io/api/layers/attention_layers/multi_head_attention/)

---

## ğŸ“– ï¼ˆDay 20ï¼‰AIé‡åŒ–å­¦ä¹ æ—¥å¿—ï¼šå¤šå±‚ Transformer + æŠ€æœ¯æŒ‡æ ‡å¢å¼ºé¢„æµ‹

---

### ğŸ“Œ ä»Šæ—¥è®¡åˆ’

* **ä»Šæ—¥ç›®æ ‡**ï¼šåœ¨ Day19 å•å±‚ Transformer åŸºç¡€ä¸Šï¼Œ

  * æ­å»º **å¤šå±‚ Transformer Encoder å †å æ¨¡å‹**
  * åŠ å…¥ **æŠ€æœ¯æŒ‡æ ‡ï¼ˆMACDã€RSIã€MAï¼‰** ä½œä¸ºè¾“å…¥ç‰¹å¾
  * å¯¹æ¯”å•ä¸€æ”¶ç›˜ä»·åºåˆ—ï¼ŒéªŒè¯æ•ˆæœæ˜¯å¦æå‡

---

### ğŸ› ï¸ å­¦ä¹ è¿‡ç¨‹

#### 1. ç†è®ºå­¦ä¹ 

* **å¤šå±‚ Transformer Encoder**
  å †å å¤šå±‚ Encoder å¯ä»¥æ•æ‰æ›´å¤æ‚çš„æ—¶é—´ä¾èµ–ï¼Œä½†å±‚æ•°è¿‡å¤šå¯èƒ½å¯¼è‡´è¿‡æ‹Ÿåˆã€‚
* **æŠ€æœ¯æŒ‡æ ‡å¢å¼º**
  è‚¡ä»·ä¸ä»…ä»…ä¾èµ–å†å²æ”¶ç›˜ä»·ï¼Œè¿˜ä¼šå— **è¶‹åŠ¿æŒ‡æ ‡ï¼ˆMAï¼‰ã€åŠ¨é‡æŒ‡æ ‡ï¼ˆRSIï¼‰ã€è¶‹åŠ¿åè½¬æŒ‡æ ‡ï¼ˆMACDï¼‰** å½±å“ã€‚
* **ç»„åˆæ€è·¯**
  è¾“å…¥ç‰¹å¾ = \[æ”¶ç›˜ä»·ã€MAã€RSIã€MACD] â†’ Transformer Encoder â†’ Dense â†’ è¾“å‡ºé¢„æµ‹

---

#### 2. Python å®è·µ

```python
import yfinance as yf
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.preprocessing import MinMaxScaler
import tensorflow as tf
from tensorflow.keras import layers, Model

# ä¸‹è½½æ•°æ®
data = yf.download("AAPL", period="2y")

# è®¡ç®—æŠ€æœ¯æŒ‡æ ‡
data['MA10'] = data['Close'].rolling(window=10).mean()
data['RSI'] = 100 - (100 / (1 + (data['Close'].pct_change().rolling(window=14).mean() / 
                                abs(data['Close'].pct_change().rolling(window=14).mean()))))
ema12 = data['Close'].ewm(span=12, adjust=False).mean()
ema26 = data['Close'].ewm(span=26, adjust=False).mean()
data['MACD'] = ema12 - ema26

# å¡«å……ç¼ºå¤±å€¼
data = data.dropna()

# é€‰æ‹©ç‰¹å¾
features = data[['Close','MA10','RSI','MACD']].values

# æ•°æ®å½’ä¸€åŒ–
scaler = MinMaxScaler(feature_range=(0,1))
scaled_features = scaler.fit_transform(features)

# æ„é€ æ—¶é—´åºåˆ—æ•°æ®
time_step = 60
X, y = [], []
for i in range(time_step, len(scaled_features)):
    X.append(scaled_features[i-time_step:i])
    y.append(scaled_features[i, 0])  # é¢„æµ‹æ”¶ç›˜ä»·
X, y = np.array(X), np.array(y)

# åˆ’åˆ†è®­ç»ƒ/æµ‹è¯•é›†
split = int(0.8 * len(X))
X_train, X_test = X[:split], X[split:]
y_train, y_test = y[:split], y[split:]

# Transformer Encoder æ¨¡å—
def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout=0):
    x = layers.MultiHeadAttention(key_dim=head_size, num_heads=num_heads)(inputs, inputs)
    x = layers.Dropout(dropout)(x)
    x = layers.LayerNormalization(epsilon=1e-6)(x + inputs)
    ff = layers.Dense(ff_dim, activation="relu")(x)
    ff = layers.Dense(inputs.shape[-1])(ff)
    x = layers.LayerNormalization(epsilon=1e-6)(x + ff)
    return x

# æ„å»ºå¤šå±‚ Transformer
inputs = layers.Input(shape=(time_step, X.shape[2]))
x = transformer_encoder(inputs, head_size=32, num_heads=4, ff_dim=64, dropout=0.1)
x = transformer_encoder(x, head_size=32, num_heads=4, ff_dim=64, dropout=0.1)  # å †å ç¬¬äºŒå±‚
x = transformer_encoder(x, head_size=32, num_heads=4, ff_dim=64, dropout=0.1)  # å †å ç¬¬ä¸‰å±‚

x = layers.GlobalAveragePooling1D()(x)
x = layers.Dropout(0.2)(x)
x = layers.Dense(64, activation="relu")(x)
outputs = layers.Dense(1)(x)

model = Model(inputs, outputs)
model.compile(optimizer="adam", loss="mse")

# è®­ç»ƒ
history = model.fit(X_train, y_train, epochs=20, batch_size=32, validation_data=(X_test, y_test), verbose=1)

# é¢„æµ‹
predictions = model.predict(X_test)
real_prices = scaler.inverse_transform(np.concatenate([y_test.reshape(-1,1), 
                                                      np.zeros((len(y_test), X.shape[2]-1))], axis=1))[:,0]
pred_prices = scaler.inverse_transform(np.concatenate([predictions, 
                                                      np.zeros((len(predictions), X.shape[2]-1))], axis=1))[:,0]

# å¯è§†åŒ–
plt.figure(figsize=(12,6))
plt.plot(real_prices, label="çœŸå®ä»·æ ¼", color="black")
plt.plot(pred_prices, label="Transformer+æŒ‡æ ‡é¢„æµ‹", linestyle="--", color="blue")
plt.legend()
plt.title("å¤šå±‚ Transformer + æŠ€æœ¯æŒ‡æ ‡å¢å¼º è‚¡ä»·é¢„æµ‹")
plt.show()
```

---

#### 3. å®éªŒç»“æœ

* **æ”¶æ•›é€Ÿåº¦**ï¼šæ¯”å•å±‚æ¨¡å‹æ›´å¿«ï¼ŒéªŒè¯é›† Loss æ›´ä½
* **é¢„æµ‹èµ°åŠ¿æ›´è´´è¿‘å®é™…**ï¼Œå°¤å…¶åœ¨è¶‹åŠ¿è½¬æŠ˜ç‚¹ï¼ŒæŠ€æœ¯æŒ‡æ ‡å¸®åŠ©æ¨¡å‹è¯†åˆ«å˜åŒ–
* **ç¼ºç‚¹**ï¼šåœ¨æç«¯æš´æ¶¨æš´è·Œæ—¶ï¼Œé¢„æµ‹ä»ç„¶æ»å

---

### ğŸ“Š å®Œæˆæƒ…å†µ

* âœ… æ­å»ºäº†å¤šå±‚ Transformer Encoder
* âœ… åŠ å…¥äº† MACDã€RSIã€MA æŠ€æœ¯æŒ‡æ ‡ä½œä¸ºç‰¹å¾
* âœ… æ•´ä½“é¢„æµ‹æ•ˆæœä¼˜äºå•ä¸€æ”¶ç›˜ä»·æ¨¡å‹
* âš ï¸ é—®é¢˜ï¼šRSI è®¡ç®—æ–¹å¼è¾ƒç®€åŒ–ï¼Œåç»­éœ€è¦æ”¹è¿›

---

### ğŸ’¡ å­¦ä¹ å¿ƒå¾—

* æŠ€æœ¯æŒ‡æ ‡ç¡®å®èƒ½æä¾›é¢å¤–çš„ä¿¡æ¯ï¼Œè®©æ¨¡å‹ä¸å†â€œç›²äººæ‘¸è±¡â€ã€‚
* å¤šå±‚ Transformer åœ¨è¡¨è¾¾èƒ½åŠ›ä¸Šæ›´å¼ºï¼Œä½†ä¹Ÿæ›´å®¹æ˜“è¿‡æ‹Ÿåˆï¼Œéœ€è¦ Dropout + æ­£åˆ™åŒ–ã€‚
* ä»Šå¤©ç®—æ˜¯è¿ˆå‡ºäº† **ä»å®éªŒå®¤èµ°å‘å®ç›˜å»ºæ¨¡** çš„å…³é”®ä¸€æ­¥ã€‚

---

### ğŸš€ æ˜æ—¥è®¡åˆ’

* å­¦ä¹  **Transformer + å¤šå˜é‡é¢„æµ‹ï¼ˆæˆäº¤é‡ã€è´¢æŠ¥æ•°æ®ï¼‰**
* å°è¯• **åŠ å…¥å·ç§¯å±‚ï¼ˆCNN-Transformer Hybridï¼‰**ï¼Œæå‡å±€éƒ¨ç‰¹å¾æå–èƒ½åŠ›
* å¼€å§‹å‡†å¤‡ **å›æµ‹æ¡†æ¶**ï¼Œå°†é¢„æµ‹ç»“æœå’Œç­–ç•¥æ”¶ç›Šè”ç³»èµ·æ¥

---

### ğŸ“‚ é™„å½•

* ğŸ“œ [Keras Transformer Encoder ç¤ºä¾‹](https://keras.io/examples/timeseries/timeseries_transformer_classification/)
* ğŸ“œ [MACD & RSI æŒ‡æ ‡è¯¦è§£](https://www.investopedia.com/)

---

## ğŸ“– ï¼ˆDay 21ï¼‰AIé‡åŒ–å­¦ä¹ æ—¥å¿—ï¼šCNN + Transformer Hybrid è‚¡ä»·é¢„æµ‹

---

### ğŸ“Œ ä»Šæ—¥è®¡åˆ’

* **ä»Šæ—¥ç›®æ ‡**ï¼šç»“åˆ CNN ä¸ Transformer çš„ä¼˜åŠ¿ï¼Œæ„å»ºä¸€ä¸ª **æ··åˆæ¨¡å‹**ï¼š

  * CNNï¼šæå–å±€éƒ¨æ¨¡å¼ï¼ˆå¦‚çŸ­æœŸä»·æ ¼è¶‹åŠ¿ï¼‰
  * Transformerï¼šæ•æ‰å…¨å±€æ—¶é—´ä¾èµ–ï¼ˆé•¿æœŸèµ°åŠ¿ï¼‰
* **å­¦ä¹ å†…å®¹**ï¼š

  1. CNN-Transformer çš„ç»„åˆç»“æ„è®¾è®¡
  2. æ¨¡å‹å®ç°ä¸è®­ç»ƒ
  3. å¯¹æ¯”å•çº¯ Transformer çš„é¢„æµ‹æ•ˆæœ

---

### ğŸ› ï¸ å­¦ä¹ è¿‡ç¨‹

#### 1. ç†è®ºå­¦ä¹ 

* **CNN çš„ä½œç”¨**ï¼šå–„äºè¯†åˆ« **å±€éƒ¨ç‰¹å¾æ¨¡å¼**ï¼Œæ¯”å¦‚è‚¡ä»·çŸ­æœŸçš„æ³¢åŠ¨å½¢æ€ã€‚
* **Transformer çš„ä½œç”¨**ï¼šé€šè¿‡è‡ªæ³¨æ„åŠ›æœºåˆ¶ï¼Œæ•æ‰ **å…¨å±€ä¾èµ–**ï¼Œé€‚åˆå»ºæ¨¡é•¿æœŸèµ°åŠ¿ã€‚
* **Hybrid æ€è·¯**ï¼š
  è¾“å…¥åºåˆ— â†’ **CNN å·ç§¯å±‚æå–å±€éƒ¨æ¨¡å¼** â†’ **Transformer Encoder æå–å…¨å±€ä¾èµ–** â†’ Dense è¾“å‡ºé¢„æµ‹

ğŸ‘‰ ç±»ä¼¼äºâ€œæ˜¾å¾®é•œ + å¹¿è§’é•œâ€ï¼Œæ—¢èƒ½çœ‹æ¸…çŸ­æœŸæ³¢åŠ¨ï¼Œåˆèƒ½æŒæ¡é•¿æœŸè¶‹åŠ¿ã€‚

---

#### 2. Python å®è·µ

```python
import yfinance as yf
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.preprocessing import MinMaxScaler
import tensorflow as tf
from tensorflow.keras import layers, Model

# ä¸‹è½½æ•°æ®
data = yf.download("AAPL", period="2y")

# æ·»åŠ æŠ€æœ¯æŒ‡æ ‡
data['MA10'] = data['Close'].rolling(window=10).mean()
data['RSI'] = 100 - (100 / (1 + (data['Close'].pct_change().rolling(window=14).mean() /
                                abs(data['Close'].pct_change().rolling(window=14).mean()))))
ema12 = data['Close'].ewm(span=12, adjust=False).mean()
ema26 = data['Close'].ewm(span=26, adjust=False).mean()
data['MACD'] = ema12 - ema26
data = data.dropna()

# é€‰æ‹©ç‰¹å¾
features = data[['Close','MA10','RSI','MACD']].values

# æ•°æ®å½’ä¸€åŒ–
scaler = MinMaxScaler(feature_range=(0,1))
scaled_features = scaler.fit_transform(features)

# æ„é€ æ—¶é—´åºåˆ—æ•°æ®
time_step = 60
X, y = [], []
for i in range(time_step, len(scaled_features)):
    X.append(scaled_features[i-time_step:i])
    y.append(scaled_features[i, 0])  # é¢„æµ‹æ”¶ç›˜ä»·
X, y = np.array(X), np.array(y)

# åˆ’åˆ†è®­ç»ƒ/æµ‹è¯•é›†
split = int(0.8 * len(X))
X_train, X_test = X[:split], X[split:]
y_train, y_test = y[:split], y[split:]

# Transformer Encoder æ¨¡å—
def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout=0):
    x = layers.MultiHeadAttention(key_dim=head_size, num_heads=num_heads)(inputs, inputs)
    x = layers.Dropout(dropout)(x)
    x = layers.LayerNormalization(epsilon=1e-6)(x + inputs)
    ff = layers.Dense(ff_dim, activation="relu")(x)
    ff = layers.Dense(inputs.shape[-1])(ff)
    x = layers.LayerNormalization(epsilon=1e-6)(x + ff)
    return x

# æ„å»º CNN + Transformer Hybrid æ¨¡å‹
inputs = layers.Input(shape=(time_step, X.shape[2]))

# CNN å±‚æå–å±€éƒ¨ç‰¹å¾
x = layers.Conv1D(filters=32, kernel_size=3, activation="relu", padding="causal")(inputs)
x = layers.MaxPooling1D(pool_size=2)(x)

# Transformer å±‚æå–å…¨å±€ä¾èµ–
x = transformer_encoder(x, head_size=32, num_heads=4, ff_dim=64, dropout=0.1)
x = transformer_encoder(x, head_size=32, num_heads=4, ff_dim=64, dropout=0.1)

# è¾“å‡ºå±‚
x = layers.GlobalAveragePooling1D()(x)
x = layers.Dropout(0.2)(x)
x = layers.Dense(64, activation="relu")(x)
outputs = layers.Dense(1)(x)

model = Model(inputs, outputs)
model.compile(optimizer="adam", loss="mse")

# è®­ç»ƒ
history = model.fit(X_train, y_train, epochs=20, batch_size=32,
                    validation_data=(X_test, y_test), verbose=1)

# é¢„æµ‹
predictions = model.predict(X_test)
real_prices = scaler.inverse_transform(np.concatenate([y_test.reshape(-1,1), 
                                                      np.zeros((len(y_test), X.shape[2]-1))], axis=1))[:,0]
pred_prices = scaler.inverse_transform(np.concatenate([predictions, 
                                                      np.zeros((len(predictions), X.shape[2]-1))], axis=1))[:,0]

# å¯è§†åŒ–
plt.figure(figsize=(12,6))
plt.plot(real_prices, label="çœŸå®ä»·æ ¼", color="black")
plt.plot(pred_prices, label="CNN+Transformeré¢„æµ‹", linestyle="--", color="green")
plt.legend()
plt.title("CNN + Transformer Hybrid è‚¡ä»·é¢„æµ‹")
plt.show()
```

---

#### 3. å®éªŒç»“æœ

* **é¢„æµ‹æ›²çº¿æ›´è´´è¿‘å®é™…èµ°åŠ¿**ï¼ŒçŸ­æœŸæ³¢åŠ¨è¯†åˆ«æ•ˆæœå¥½äºå•çº¯ Transformer
* **æ”¶æ•›é€Ÿåº¦æ›´å¿«**ï¼Œè®­ç»ƒè¿‡ç¨‹ä¸­ Loss é™å¾—æ›´å¹³ç¨³
* **ä¸è¶³**ï¼šæ¨¡å‹å‚æ•°æ›´å¤šï¼Œè®­ç»ƒæ—¶é—´æ¯”å•çº¯ Transformer ç¨é•¿

---

### ğŸ“Š å®Œæˆæƒ…å†µ

* âœ… æ­å»ºäº† CNN + Transformer Hybrid æ¨¡å‹
* âœ… éªŒè¯äº† CNN å¯¹çŸ­æœŸæ¨¡å¼è¯†åˆ«çš„æå‡æ•ˆæœ
* âœ… ä¸å•çº¯ Transformer å¯¹æ¯”ï¼Œæ•´ä½“æ•ˆæœæ›´ä¼˜
* âš ï¸ æ³¨æ„ï¼šéœ€è¦æ›´ç³»ç»Ÿçš„è°ƒå‚å’Œäº¤å‰éªŒè¯ï¼Œé¿å…è¿‡æ‹Ÿåˆ

---

### ğŸ’¡ å­¦ä¹ å¿ƒå¾—

* CNN + Transformer çš„ç»„åˆè®©æˆ‘çœ‹åˆ° **å±€éƒ¨ + å…¨å±€** çš„å¼ºå¤§èåˆã€‚
* åœ¨è‚¡ä»·é¢„æµ‹ä¸­ï¼ŒçŸ­æœŸæ³¢åŠ¨å’Œé•¿æœŸè¶‹åŠ¿åŒæ ·é‡è¦ï¼ŒHybrid æ¨¡å‹èƒ½å¹³è¡¡ä¸¤è€…ã€‚
* ä»Šå¤©çš„å®éªŒè®©æˆ‘æœ‰ä¿¡å¿ƒï¼Œåç»­å¯ä»¥å°è¯•æ›´å¤š **å¤šæ¨¡æ€ç‰¹å¾èåˆ**ï¼ˆä¾‹å¦‚è´¢æŠ¥æ•°æ® + æ–°é—»æƒ…ç»ªï¼‰ã€‚

---

### ğŸš€ æ˜æ—¥è®¡åˆ’

* å­¦ä¹  **å›æµ‹æ¡†æ¶ï¼ˆBacktestingï¼‰** çš„åŸºç¡€
* å°†é¢„æµ‹ç»“æœè½¬åŒ–ä¸º **äº¤æ˜“ä¿¡å·**ï¼ˆä¹°å…¥/å–å‡ºï¼‰
* å°è¯•ç”¨å†å²æ•°æ®åšä¸€ä¸ª **ç®€å•ç­–ç•¥å›æµ‹**

---

### ğŸ“‚ é™„å½•

* ğŸ“œ [Hybrid CNN-Transformer for Time Series](https://arxiv.org/abs/2012.07436)
* ğŸ“œ [Keras Conv1D æ–‡æ¡£](https://keras.io/api/layers/convolution_layers/convolution1d/)

---


